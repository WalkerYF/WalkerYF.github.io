<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog</title>
  
  <subtitle>My Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wwyf.github.io/"/>
  <updated>2019-01-15T13:48:00.257Z</updated>
  <id>https://wwyf.github.io/</id>
  
  <author>
    <name>wyf</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>高性能计算基础-复习4-OpenMP</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A04-OpenMP/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习4-OpenMP/</id>
    <published>2019-01-15T13:44:24.000Z</published>
    <updated>2019-01-15T13:48:00.257Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习4-OpenMP"><a href="#HPC复习4-OpenMP" class="headerlink" title="HPC复习4-OpenMP"></a>HPC复习4-OpenMP</h1><ol><li>OpenMP简介</li><li>OpenMP编译制导</li><li>OpenMP库函数</li><li>OpenMP环境变量</li><li>OpenMP示例–性能改善</li></ol><a id="more"></a><h2 id="OpenMP简介"><a href="#OpenMP简介" class="headerlink" title="OpenMP简介"></a>OpenMP简介</h2><ol><li>是一种基于线程的并行编程模型<ol><li>编译制导</li><li>运行库函数</li><li>环境变量</li></ol></li><li>采用<strong>Fork-Join</strong>并行执行方式<ol><li>OpenMP程序开始于一个单独的主线程（Master Thread），然后主线程一直串行执行，直到遇见第一个并行域(Parallel Region)，然后开始并行执行并行域。并行域代码执行完后再回到主线程，直到遇到下一个并行域，以此类推，直至程序运行结束。</li><li><img src="https://lh3.googleusercontent.com/-2y771w5Yl0Y/XD3UJrbnJDI/AAAAAAAAN1A/sj3jk-OsMMYb-x7sjX6OEqeoPdSZ0EmaACHMYCw/s0/Acrobat_2019-01-15_20-37-57.png" alt=""></li></ol></li></ol><h2 id="OpenMP-编译制导"><a href="#OpenMP-编译制导" class="headerlink" title="OpenMP 编译制导"></a>OpenMP 编译制导</h2><p>编译制导语句格式：<strong>制导标识符 制导名称 [Cluase,…]</strong></p><h3 id="并行域制导"><a href="#并行域制导" class="headerlink" title="并行域制导"></a>并行域制导</h3><blockquote><p>一个并行域就是一个能被多个线程并行执行的程序段</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> ompparallel [clauses]</span></span><br><span class="line">&#123;</span><br><span class="line">BLOCK</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>在并行域结尾有一个隐式同步（barrier）。</li><li>子句（clause）用来说明并行域的附加信息。</li><li>在C/C++中，子句间用空格分开。（Fortran语言中，子句间用逗号或空格分隔）</li></ol><p><img src="https://lh3.googleusercontent.com/-CvLoMA9lHL0/XD3h6muXSmI/AAAAAAAAN1M/mu9RelbjSocH1CzKVCmrsuxTsTSGf9mpACHMYCw/s0/Acrobat_2019-01-15_21-36-40.png" alt=""></p><h3 id="数据访问相关子句"><a href="#数据访问相关子句" class="headerlink" title="数据访问相关子句"></a>数据访问相关子句</h3><p><img src="https://lh3.googleusercontent.com/-0QjMf3-Fjjk/XD3iFQgDyrI/AAAAAAAAN1Q/5yFImqC7IDkjChF_xy5GmiZ227IFTUtcwCHMYCw/s0/Acrobat_2019-01-15_21-37-20.png" alt=""></p><blockquote><p>如何决定哪些变量是共享哪些是私有？</p><ol><li>通常循环变量、临时变量、写变量一般是私有的；</li><li>数组变量、仅用于读的变量通常是共享的。默认时为公有</li></ol></blockquote><h3 id="并行域结构：reduction子句"><a href="#并行域结构：reduction子句" class="headerlink" title="并行域结构：reduction子句"></a>并行域结构：reduction子句</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inti,myid,n; <span class="keyword">double</span> a[][];</span><br><span class="line"><span class="keyword">double</span> sum;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel reduction(+: sum), private(i, myid)</span></span><br><span class="line">&#123;</span><br><span class="line">    myid=omp_get_thread_num();</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;n;i++)</span><br><span class="line">    sum=sum+a[i][myid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>特别说明：要理解reduction操作</p><ol><li>在reduction子句中，编译器为每个线程创建变量sum的私有副本。当循环完成后，将这些值加在一起并把结果放到原始的变量sum中;</li><li>Reduction中的op操作必须满足算术结合律和交换律。</li></ol></blockquote><p>剩下的，详细可见总结或文档。</p><h3 id="任务划分并行制导"><a href="#任务划分并行制导" class="headerlink" title="任务划分并行制导"></a>任务划分并行制导</h3><ol><li>并行for循环制导<ol><li>schedule 调度子句<ol><li>static</li><li>dynamic</li><li>guided</li><li>runtime</li></ol></li></ol></li><li>并行sections制导</li><li>single和master制导</li><li>其他制导</li></ol><h3 id="同步制导"><a href="#同步制导" class="headerlink" title="同步制导"></a>同步制导</h3><ol><li>barrier制导</li><li>nowait制导</li><li>critical制导</li><li>atomic制导</li><li>lock例程</li><li>flush制导：高速缓存会刷新恢复到内存，影响性能</li></ol><h2 id="OpenMP库函数"><a href="#OpenMP库函数" class="headerlink" title="OpenMP库函数"></a>OpenMP库函数</h2><table><thead><tr><th>函数名</th><th>作用</th></tr></thead><tbody><tr><td><code>int omp_get_num_threads()</code></td><td>得到线程队列中的线程数</td></tr><tr><td><code>int omp_get_thread_num()</code></td><td>得到执行线程的线程号：</td></tr><tr><td><code>void omp_set_num_threads(4)</code></td><td>设定执行线程的数量为4</td></tr><tr><td><code>double omp_get_wtime(void)</code></td><td>返回挂钟“自过去任意时刻以来”经过的时间（秒）</td></tr><tr><td><code>double omp_get_wtick(void);</code></td><td>返回连续时钟滴答声之间的秒数。</td></tr></tbody></table><h2 id="OpenMP环境变量"><a href="#OpenMP环境变量" class="headerlink" title="OpenMP环境变量"></a>OpenMP环境变量</h2><table><thead><tr><th>环境变量名称</th><th>作用</th></tr></thead><tbody><tr><td>OMP_NUM_THREADS</td><td>设定最大线程数</td></tr><tr><td>OMP_SCHEDULE</td><td>调度方式 “DYNAMIC,4”</td></tr><tr><td>OMP_DYNAMIC</td><td>是否动态设定并行域执行的线程数</td></tr><tr><td>OMP_NESTED</td><td>确定是否可以并行嵌套</td></tr></tbody></table><h2 id="OpenMP示例–性能改善"><a href="#OpenMP示例–性能改善" class="headerlink" title="OpenMP示例–性能改善"></a>OpenMP示例–性能改善</h2><h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><p>如何处理循环中存在的依赖关系？例子如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;m;i++)</span><br><span class="line"><span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">a[i][j]=<span class="number">2.0</span>*a[i<span class="number">-1</span>][j];</span><br></pre></td></tr></table></figure><p>可以变为(交换循环位置)；</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;m;i++)</span><br><span class="line">a[i][j]=<span class="number">2.0</span>*a[i<span class="number">-1</span>][j];</span><br></pre></td></tr></table></figure><h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><p>下面这一个循环是存在循环依赖的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">2</span>;i&lt;<span class="number">20</span>;i++)</span><br><span class="line">a[i]=a[i<span class="number">-2</span>]+<span class="number">4</span> ;</span><br></pre></td></tr></table></figure><p>不过我们可以将这一个循环分解为多个循环，这多个循环间不存在依赖关系</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">2</span>;i&lt;<span class="number">20</span>;i=i+<span class="number">2</span>)</span><br><span class="line">a[i]=a[i<span class="number">-2</span>]+<span class="number">4</span> ;</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">3</span>;i&lt;<span class="number">20</span>;i=i+<span class="number">2</span>)</span><br><span class="line">a[i]=a[i<span class="number">-2</span>]+<span class="number">4</span> ;</span><br></pre></td></tr></table></figure><h2 id="制导指令总结"><a href="#制导指令总结" class="headerlink" title="制导指令总结"></a>制导指令总结</h2><h3 id="并行域指令"><a href="#并行域指令" class="headerlink" title="并行域指令"></a>并行域指令</h3><p><img src="https://lh3.googleusercontent.com/-Qq6sgKD2ji0/XD3i5oH9VvI/AAAAAAAAN1c/Ry8Ay8PLx0ICJfQERdUhafUk00ta3cIZwCHMYCw/s0/Acrobat_2019-01-15_21-40-53.png" alt=""></p><h3 id="工作共享指令"><a href="#工作共享指令" class="headerlink" title="工作共享指令"></a>工作共享指令</h3><p><img src="https://lh3.googleusercontent.com/-JhjPfP392W0/XD3i8nOBkoI/AAAAAAAAN1g/b0rFfhaRdXE7qo4bSY36EMmF0PoL8DCDQCHMYCw/s0/Acrobat_2019-01-15_21-41-06.png" alt=""></p><h3 id="并行域与工作共享指令的结合"><a href="#并行域与工作共享指令的结合" class="headerlink" title="并行域与工作共享指令的结合"></a>并行域与工作共享指令的结合</h3><p><img src="https://lh3.googleusercontent.com/-y7KBSiWaDl4/XD3i_TPH8OI/AAAAAAAAN1k/Q3pjVu3MC-YArhkz1VxhLnJFlenauZjhgCHMYCw/s0/Acrobat_2019-01-15_21-41-17.png" alt=""></p><h3 id="同步指令"><a href="#同步指令" class="headerlink" title="同步指令"></a>同步指令</h3><p><img src="https://lh3.googleusercontent.com/-y7KBSiWaDl4/XD3i_TPH8OI/AAAAAAAAN1k/Q3pjVu3MC-YArhkz1VxhLnJFlenauZjhgCHMYCw/s0/Acrobat_2019-01-15_21-41-17.png" alt=""></p><h3 id="数据环境指令"><a href="#数据环境指令" class="headerlink" title="数据环境指令"></a>数据环境指令</h3><p><img src="https://lh3.googleusercontent.com/-Ccf5agIIOL8/XD3jFewuXmI/AAAAAAAAN1s/825ZKEyguzkYXD8gA7sOIZjctwbZf9CdwCHMYCw/s0/Acrobat_2019-01-15_21-41-39.png" alt=""></p><h2 id="OpenMP子句总结"><a href="#OpenMP子句总结" class="headerlink" title="OpenMP子句总结"></a>OpenMP子句总结</h2><h3 id="数据作用域属性子句"><a href="#数据作用域属性子句" class="headerlink" title="数据作用域属性子句"></a>数据作用域属性子句</h3><p><img src="https://lh3.googleusercontent.com/-JQovoZ2MIzk/XD3jWTZcajI/AAAAAAAAN2A/jrZd8dFECb4lNtCWV3poWFlnx2709pZOACHMYCw/s0/Acrobat_2019-01-15_21-42-48.png" alt=""></p><p><img src="https://lh3.googleusercontent.com/-RY1UDZes6Q4/XD3jZY1-7PI/AAAAAAAAN2E/1-F2Y4dXSngZLhY54tNDClmQt-uVcFF-ACHMYCw/s0/Acrobat_2019-01-15_21-43-01.png" alt=""></p><h3 id="其他子句"><a href="#其他子句" class="headerlink" title="其他子句"></a>其他子句</h3><p><img src="https://lh3.googleusercontent.com/-sfXhe5-b9qg/XD3jcESZ9CI/AAAAAAAAN2I/tN-9ENQk29AezWKTJLkPA9IcBcboBEHHQCHMYCw/s0/Acrobat_2019-01-15_21-43-12.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习4-OpenMP&quot;&gt;&lt;a href=&quot;#HPC复习4-OpenMP&quot; class=&quot;headerlink&quot; title=&quot;HPC复习4-OpenMP&quot;&gt;&lt;/a&gt;HPC复习4-OpenMP&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;OpenMP简介&lt;/li&gt;
&lt;li&gt;OpenMP编译制导&lt;/li&gt;
&lt;li&gt;OpenMP库函数&lt;/li&gt;
&lt;li&gt;OpenMP环境变量&lt;/li&gt;
&lt;li&gt;OpenMP示例–性能改善&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习7.4-归约操作优化</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A07-4-%E5%BD%92%E7%BA%A6%E6%93%8D%E4%BD%9C%E4%BC%98%E5%8C%96/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习7-4-归约操作优化/</id>
    <published>2019-01-15T12:02:10.000Z</published>
    <updated>2019-01-15T12:02:39.602Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习7-4-并行归约优化"><a href="#HPC复习7-4-并行归约优化" class="headerlink" title="HPC复习7.4-并行归约优化"></a>HPC复习7.4-并行归约优化</h1><p>这里会有归约操作的7种优化版本。</p><p><img src="https://lh3.googleusercontent.com/-8rV-AMsd_Rw/XD2sO0WhFPI/AAAAAAAANzs/dR5OQxmXKt4l3HpoRmnq4Oh9VAf-hN7twCHMYCw/s0/Acrobat_2019-01-15_17-47-39.png" alt=""></p><p>7种方法的加速情况如下：</p><p><img src="https://lh3.googleusercontent.com/-ZARtsyKmBik/XD2s_jh4RUI/AAAAAAAANz0/ZVLxWOhn41MEkuP9fTrLlLwKSkEGc22awCHMYCw/s0/Acrobat_2019-01-15_17-50-54.png" alt=""></p><a id="more"></a><h2 id="Interleaved-Addressing-with-divergent-branching"><a href="#Interleaved-Addressing-with-divergent-branching" class="headerlink" title="Interleaved Addressing with divergent branching"></a>Interleaved Addressing with divergent branching</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce0</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata)</span> </span>&#123;</span><br><span class="line"><span class="keyword">extern</span> __shared__ <span class="keyword">int</span> sdata[];</span><br><span class="line">    <span class="comment">// each thread loads one element from global to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i = blockIdx.x*blockDim.x + threadIdx.x;</span><br><span class="line">    sdata[tid] = g_idata[i]; <span class="comment">// 合并访存，读取一个block的数据到share memory中</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s &lt; blockDim.x; s *= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid % (<span class="number">2</span>*s) == <span class="number">0</span>) &#123;</span><br><span class="line">            sdata[tid] += sdata[tid + s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/-hfUJjKZ4Ksw/XD2vIHV9BzI/AAAAAAAAN0A/gLS3oE0NzqEKGI1O8hjvAI_0jFSTe3wKQCHMYCw/s0/Acrobat_2019-01-15_17-59-59.png" alt=""></p><h3 id="存在的问题："><a href="#存在的问题：" class="headerlink" title="存在的问题："></a>存在的问题：</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s &lt; blockDim.x; s *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid % (<span class="number">2</span>*s) == <span class="number">0</span>) &#123; <span class="comment">// 以warp的运行想一想，就会发现，这个是发散的</span></span><br><span class="line">        sdata[tid] += sdata[tid + s];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Interleaved-address-with-bank-conflict"><a href="#Interleaved-address-with-bank-conflict" class="headerlink" title="Interleaved address with bank conflict"></a>Interleaved address with bank conflict</h2><p>对上面的问题进行优化，将对应的for循环改成下面的循环：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s &lt; blockDim.x; s *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">2</span> * s * tid;</span><br><span class="line">    <span class="keyword">if</span> (index &lt; blockDim.x) &#123;</span><br><span class="line">    sdata[index] += sdata[index + s];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看了老半天终于看懂了</p><p>第一次循环：</p><p>indexs:0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32，取其中的</p><p>0,2,4,6,8,10,12,14，也就是前8个线程去计算</p><p>第二次循环</p><p>indexs:0,4,8,12,16,….，取前四个线程去计算</p><p><img src="https://lh3.googleusercontent.com/-mB5VGBUtzb4/XD2w5t6nmEI/AAAAAAAAN0M/dzqs-ASHP2syD1k7QFzuprgHYpv0fHjwACHMYCw/s0/Acrobat_2019-01-15_18-07-34.png" alt=""></p><h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>数组可能是很大的，考虑到有16个bank，这里在归约的时候，都是以2的倍数在增加，及其容易发生bank冲突。</p><p>如，在第二次循环中，会去读取这些index对应的数据</p><p>0,4,8,12,16,20,24,28,32，。。。就会发生bank冲突了。</p><h2 id="Sequential-Addressing"><a href="#Sequential-Addressing" class="headerlink" title="Sequential Addressing"></a>Sequential Addressing</h2><p>这一个优化就好多啦,不会发生bank冲突，因为一定是顺序写shared memory，看一看示意图，很容易懂！</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; s) &#123;</span><br><span class="line">        sdata[tid] += sdata[tid + s];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/-fIBj8uEKW8I/XD2yyRsUFcI/AAAAAAAAN0c/xi3CeNJWtXs9Uvgg14Yrsxhx228gx3LcQCHMYCw/s0/Acrobat_2019-01-15_18-15-38.png" alt=""></p><h2 id="First-Add-During-Load"><a href="#First-Add-During-Load" class="headerlink" title="First Add During Load"></a>First Add During Load</h2><p>关键：在装入shared memory时做第一次加法</p><p>从代码中可以看出思路：在读取数组中的值时，原本是以一个blockDim.x为单位读取，每个block读取与blockDim.x相当的元素，放到shared memory中，现在每个block会读取两个blockDim.x大小的数组，然后在写进shared memory的时候就做一次加法。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// perform first level of reduction,</span></span><br><span class="line"><span class="comment">// reading from global memory, writing to shared memory</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> i = blockIdx.x*(blockDim.x*<span class="number">2</span>) + threadIdx.x;</span><br><span class="line">sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure><h3 id="存在的问题：-1"><a href="#存在的问题：-1" class="headerlink" title="存在的问题："></a>存在的问题：</h3><p>可能在指令的吞吐量上，有瓶颈：</p><p>尝试循环展开</p><h2 id="Unrolling-the-Last-Warp"><a href="#Unrolling-the-Last-Warp" class="headerlink" title="Unrolling the Last Warp"></a>Unrolling the Last Warp</h2><p>这里主要尝试这样的做法：</p><ol><li>当归约到只剩下32个线程需要计算的时候（也就是说只剩下一个warp了）</li><li>显式的将剩下的32个线程所需要做的事情通过循环展开的方式来完成</li></ol><p>优化主要体现在两点：</p><ol><li>节省了所有的warp的额外开销<ol><li>想想其他的warp怎么会有开销呢？当只剩下一个warp需要进行实质的工作的时候，其他warp仍然在执行循环。</li><li>这个额外开销指的是维护循环变量，检查循环条件等</li></ol></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; s)</span><br><span class="line">        sdata[tid] += sdata[tid + s];</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">32</span>)</span><br><span class="line">&#123;</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">32</span>];</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">16</span>];</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">8</span>];</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">4</span>];</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">2</span>];</span><br><span class="line">    sdata[tid] += sdata[tid + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>这里仅仅展开了最后的一个warp，能否完全展开呢？</p><h2 id="Completely-Unrolled"><a href="#Completely-Unrolled" class="headerlink" title="Completely Unrolled"></a>Completely Unrolled</h2><p>由于编译期就可以知道迭代的<code>blockDim.x</code>，因此可以通过模板的方式生成循环展开的相关代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (blockSize &gt;= <span class="number">512</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">256</span>) &#123; sdata[tid] += sdata[tid + <span class="number">256</span>]; &#125; __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (blockSize &gt;= <span class="number">256</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">128</span>) &#123; sdata[tid] += sdata[tid + <span class="number">128</span>]; &#125; __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (blockSize &gt;= <span class="number">128</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">64</span>) &#123; sdata[tid] += sdata[tid + <span class="number">64</span>]; &#125; __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">32</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">64</span>) sdata[tid] += sdata[tid + <span class="number">32</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">32</span>) sdata[tid] += sdata[tid + <span class="number">16</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">16</span>) sdata[tid] += sdata[tid + <span class="number">8</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">8</span>) sdata[tid] += sdata[tid + <span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">4</span>) sdata[tid] += sdata[tid + <span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">2</span>) sdata[tid] += sdata[tid + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的blockSize是个模板参数</p><h2 id="Multiple-Adds-Thread"><a href="#Multiple-Adds-Thread" class="headerlink" title="Multiple Adds/Thread"></a>Multiple Adds/Thread</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>combine sequential and parallel reduction</p><p>原理有点不懂。</p><p>Algorithm Cascading（算法级联）</p><p>它具体的做法是这样子的（我猜）：</p><ol><li>申请的线程可以少一些，推荐申请$O(N/logN)$个线程</li><li>在算法开始的时候，先使用串行求和，（也就是下面那一串代码），这样子就可以将问题规模缩小到与线程数的情况</li><li>后面都一样</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> i = blockIdx.x*(blockSize*<span class="number">2</span>) + threadIdx.x;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize*<span class="number">2</span>*gridDim.x;</span><br><span class="line">sdata[tid] = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; n) &#123;</span><br><span class="line">    sdata[tid] += g_idata[i] + g_idata[i+blockSize];</span><br><span class="line">    i += gridSize;</span><br><span class="line">&#125;</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>前面的优化都其实特别有意思，特别是最后一个</p><p>最后一个优化应该说在理论层面的推导会稍微多一点，所以就不容易发现。</p><p>而正因为此吧，我觉得我需要弄清楚什么是“算法级联”，如何计算并行算法的复杂度。</p><p>如何推导出来怎样的“算法级联”能够给程序加速。</p><p>最后优化的程序如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce6</span><span class="params">(<span class="keyword">int</span> *g_idata, <span class="keyword">int</span> *g_odata, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="keyword">int</span> sdata[];</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i = blockIdx.x*(blockSize*<span class="number">2</span>) + tid;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize*<span class="number">2</span>*gridDim.x;</span><br><span class="line">    sdata[tid] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123; sdata[tid] += g_idata[i] + g_idata[i+blockSize]; i += gridSize; &#125; <span class="keyword">while</span> (i &lt; n);</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">512</span>) &#123; </span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">256</span>) &#123; sdata[tid] += sdata[tid + <span class="number">256</span>]; &#125; __syncthreads(); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">256</span>) &#123; </span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">128</span>) &#123; sdata[tid] += sdata[tid + <span class="number">128</span>]; &#125; __syncthreads(); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">128</span>) &#123; </span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">64</span>) &#123; sdata[tid] += sdata[tid + <span class="number">64</span>]; &#125; __syncthreads(); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; <span class="number">32</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">64</span>) sdata[tid] += sdata[tid + <span class="number">32</span>];</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">32</span>) sdata[tid] += sdata[tid + <span class="number">16</span>];</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">16</span>) sdata[tid] += sdata[tid + <span class="number">8</span>];</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">8</span>) sdata[tid] += sdata[tid + <span class="number">4</span>];</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">4</span>) sdata[tid] += sdata[tid + <span class="number">2</span>];</span><br><span class="line">        <span class="keyword">if</span> (blockSize &gt;= <span class="number">2</span>) sdata[tid] += sdata[tid + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>优化的结果可见：</p><p><img src="https://lh3.googleusercontent.com/-bnSz_ewThXc/XD3KsyUzD9I/AAAAAAAAN0w/cTjEcJ9zFb4k_SKLLmMwUIPyAh1TANfDgCHMYCw/s0/Acrobat_2019-01-15_19-57-36.png" alt=""></p><h2 id="相关博客"><a href="#相关博客" class="headerlink" title="相关博客"></a>相关博客</h2><ol><li><a href="http://wattlebird.github.io/2013/07/20/%E5%85%B3%E4%BA%8E-CUDA-%E4%B8%AD-reduction-%E8%BF%90%E7%AE%97%E7%9A%84%E4%BC%98%E5%8C%96/" target="_blank" rel="noopener">关于 CUDA 中 reduction 运算的优化</a></li><li><a href="https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms" target="_blank" rel="noopener">Brent’s theorem</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习7-4-并行归约优化&quot;&gt;&lt;a href=&quot;#HPC复习7-4-并行归约优化&quot; class=&quot;headerlink&quot; title=&quot;HPC复习7.4-并行归约优化&quot;&gt;&lt;/a&gt;HPC复习7.4-并行归约优化&lt;/h1&gt;&lt;p&gt;这里会有归约操作的7种优化版本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-8rV-AMsd_Rw/XD2sO0WhFPI/AAAAAAAANzs/dR5OQxmXKt4l3HpoRmnq4Oh9VAf-hN7twCHMYCw/s0/Acrobat_2019-01-15_17-47-39.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;7种方法的加速情况如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-ZARtsyKmBik/XD2s_jh4RUI/AAAAAAAANz0/ZVLxWOhn41MEkuP9fTrLlLwKSkEGc22awCHMYCw/s0/Acrobat_2019-01-15_17-50-54.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习7.3-矩阵转置优化</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A07-3-%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习7-3-矩阵转置优化/</id>
    <published>2019-01-15T12:01:04.000Z</published>
    <updated>2019-01-15T12:02:39.602Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习7-3-矩阵转置优化"><a href="#HPC复习7-3-矩阵转置优化" class="headerlink" title="HPC复习7.3-矩阵转置优化"></a>HPC复习7.3-矩阵转置优化</h1><p>这一个例子使用了以下几种技巧优化了矩阵转置：</p><ol><li><p>合并访问</p></li><li><p>避免共享内存的bank conflict</p><p><img src="https://lh3.googleusercontent.com/-KenoHV2uZck/XD2rzKYgx9I/AAAAAAAANzk/fSipGdSUZRgZEL5HDX_I0JVWik5ZZOfnwCHMYCw/s0/Acrobat_2019-01-15_17-45-49.png" alt=""></p></li></ol><a id="more"></a><h2 id="NAIVE"><a href="#NAIVE" class="headerlink" title="NAIVE"></a>NAIVE</h2><p>原始的矩阵转置可见</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">transpose_naive</span><span class="params">(<span class="keyword">float</span> *odata, <span class="keyword">float</span> *idata, <span class="keyword">int</span> width, <span class="keyword">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> xIndex = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> yIndex = blockDim.y * blockIdx.y + threadIdx.y;</span><br><span class="line">    <span class="comment">// //这里xIndex对应矩阵的列号，yIndex对应矩阵的行号</span></span><br><span class="line">    <span class="keyword">if</span> (xIndex &lt; width &amp;&amp; yIndex &lt; height)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> index_in = xIndex + width * yIndex;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> index_out = yIndex + height * xIndex;</span><br><span class="line">        odata[index_out] = idata[index_in];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// //warp的排列：按threadIdx.x优先的次序</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="合并访存优化"><a href="#合并访存优化" class="headerlink" title="合并访存优化"></a>合并访存优化</h2><p>在原始的版本里，关键问题在于：</p><p><code>odata[index_out]</code>写的时候，无法合并访存：</p><p><img src="https://lh3.googleusercontent.com/-Ji3U1csOcqw/XD2mimdstWI/AAAAAAAANzA/2Ssh9Fc9_W4LrIp9VCK0KkxurBf5Q5IOgCHMYCw/s0/Acrobat_2019-01-15_17-23-22.png" alt=""></p><h3 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h3><ol><li>关键思想：使用合并访存技巧，读取一个块到shared memory中<ol><li><img src="https://lh3.googleusercontent.com/-e2xir8DtlLw/XD2rL1J5cgI/AAAAAAAANzM/IRKaftgFzF4yk62GpzPVBY_Yb7F_BEfoACHMYCw/s0/Acrobat_2019-01-15_17-43-10.png" alt=""></li></ol></li><li>在shared memory中，通过调换读取的索引，实现合并访存写回内存中<ol><li><img src="https://lh3.googleusercontent.com/-NPJTmLUBZio/XD2rWEhYLmI/AAAAAAAANzQ/je5sVWS75IIU8RL0rfLO0uHNNIzN_q02wCHMYCw/s0/Acrobat_2019-01-15_17-43-52.png" alt=""></li></ol></li></ol><h2 id="解决bank冲突"><a href="#解决bank冲突" class="headerlink" title="解决bank冲突"></a>解决bank冲突</h2><p>在上面读取shared memory中，会发生bank冲突导致读取串行化的问题。</p><p><img src="https://lh3.googleusercontent.com/-MM2-kYkf_qI/XD2rfV1YaBI/AAAAAAAANzY/8wKHxHvatQ8iPcHgw34uhA30DSJJoCXAgCHMYCw/s0/Acrobat_2019-01-15_17-44-29.png" alt=""></p><h2 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">transpose</span><span class="params">(<span class="keyword">float</span> *odata, <span class="keyword">float</span> *idata, <span class="keyword">int</span> width, <span class="keyword">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> block[(BLOCK_DIM+<span class="number">1</span>)*BLOCK_DIM];</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> xBlock = __mul24(blockDim.x, blockIdx.x);</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> yBlock = __mul24(blockDim.y, blockIdx.y);</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> xIndex = xBlock + threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> yIndex = yBlock + threadIdx.y;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> index_out, index_transpose;</span><br><span class="line">    <span class="keyword">if</span> (xIndex &lt; width &amp;&amp; yIndex &lt; height)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> index_in = __mul24(width, yIndex) + xIndex;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> index_block = __mul24(threadIdx.y, BLOCK_DIM+<span class="number">1</span>) + threadIdx.x;</span><br><span class="line">        block[index_block] = idata[index_in];</span><br><span class="line">        index_transpose = __mul24(threadIdx.x, BLOCK_DIM+<span class="number">1</span>) + threadIdx.y;</span><br><span class="line">        index_out = __mul24(height, xBlock + threadIdx.y) + yBlock + threadIdx.x;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span> (xIndex &lt; width &amp;&amp; yIndex &lt; height)</span><br><span class="line">    odata[index_out] = block[index_transpose];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习7-3-矩阵转置优化&quot;&gt;&lt;a href=&quot;#HPC复习7-3-矩阵转置优化&quot; class=&quot;headerlink&quot; title=&quot;HPC复习7.3-矩阵转置优化&quot;&gt;&lt;/a&gt;HPC复习7.3-矩阵转置优化&lt;/h1&gt;&lt;p&gt;这一个例子使用了以下几种技巧优化了矩阵转置：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;合并访问&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;避免共享内存的bank conflict&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-KenoHV2uZck/XD2rzKYgx9I/AAAAAAAANzk/fSipGdSUZRgZEL5HDX_I0JVWik5ZZOfnwCHMYCw/s0/Acrobat_2019-01-15_17-45-49.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习7.2-cuda矩阵向量乘法优化</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A07-2-cuda%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习7-2-cuda矩阵向量乘法优化/</id>
    <published>2019-01-15T11:59:38.000Z</published>
    <updated>2019-01-15T12:02:39.602Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习7-2-矩阵向量乘法优化"><a href="#HPC复习7-2-矩阵向量乘法优化" class="headerlink" title="HPC复习7.2-矩阵向量乘法优化"></a>HPC复习7.2-矩阵向量乘法优化</h1><blockquote><p>这里描述了从CPU到GPU的优化步骤，步步渐进，最终收敛于使用cuda的标准数学库。</p></blockquote><p>对于CUDA程序开发来说，优化往往是整个开发过程的核心，不同算法，不同存储器组织的程序性能往往差几十倍，本讲通过一个简单的例子来展示CUDA开发中一些重要的因素对性能的影响。</p><p>最后的优化结果可见</p><p><img src="https://lh3.googleusercontent.com/-eO3K2LRIf3Q/XD2cyoaQgII/AAAAAAAANyo/CwokcPDNYRkX3mZo8UqcINMGXtAgpmxOwCHMYCw/s0/Acrobat_2019-01-15_16-41-45.png" alt=""></p><h2 id="原始版本：串行C版本"><a href="#原始版本：串行C版本" class="headerlink" title="原始版本：串行C版本"></a>原始版本：串行C版本</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mxv</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> rowSize, <span class="keyword">const</span> <span class="keyword">int</span> columnSize,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">float</span> *matrix, <span class="keyword">const</span> <span class="keyword">float</span> *v, <span class="keyword">float</span> *r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rowSize; i++)&#123;</span><br><span class="line">        <span class="keyword">float</span> re = <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; columnSize; j++)&#123;</span><br><span class="line">        re += matrix[i*columnSize+j]*v[j];</span><br><span class="line">        &#125;</span><br><span class="line">        r[i] = re;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="CPU上的优化"><a href="#CPU上的优化" class="headerlink" title="CPU上的优化"></a>CPU上的优化</h2><p>在cpu上的优化我就不详细说了吧</p><ol><li><p>使用sse指令来计算</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mxvSSE</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> rowSize, <span class="keyword">const</span> <span class="keyword">int</span> columnSize, <span class="keyword">const</span> <span class="keyword">float</span> *matrix, <span class="keyword">const</span> <span class="keyword">float</span> *v, <span class="keyword">float</span> *r)</span></span>&#123;</span><br><span class="line">    __m128 *mv = (__m128*)v;</span><br><span class="line">    __m128 *mm = (__m128*)matrix;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rowSize; i++)&#123;</span><br><span class="line">        __m128 re = _mm_set_ps(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; columnSize/<span class="number">4</span>; j++)&#123;</span><br><span class="line">            re = _mm_add_ps(re, _mm_mul_ps(mm[i*columnSize/<span class="number">4</span>+j], mv[j]));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">float</span> __attribute((aligned(<span class="number">16</span>))) a[<span class="number">4</span>];</span><br><span class="line">        _mm_store_ps(a, re);</span><br><span class="line">        r[i] = a[<span class="number">0</span>] + a[<span class="number">1</span>] + a[<span class="number">2</span>] + a[<span class="number">3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>使用sse+openmp来优化</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mxvSSEOpenmp</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> rowSize, <span class="keyword">const</span> <span class="keyword">int</span> columnSize, <span class="keyword">float</span> *matrix, <span class="keyword">float</span> *vec, <span class="keyword">float</span> *r)</span></span>&#123;</span><br><span class="line">    __m128 *mv = (__m128*)v;</span><br><span class="line">    __m128 *mm = (__m128*)matrix;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for num_threads(2)</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rowSize; i++)&#123;</span><br><span class="line">        __m128 re = _mm_set_ps(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; columnSize/<span class="number">4</span>; j++)&#123;</span><br><span class="line">        re = _mm_add_ps(re, _mm_mul_ps(mm[i*columnSize/<span class="number">4</span>+j], mv[j]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">float</span> __attribute((aligned(<span class="number">16</span>))) a[<span class="number">4</span>];</span><br><span class="line">    _mm_store_ps(a, re);</span><br><span class="line">    r[i] = a[<span class="number">0</span>] + a[<span class="number">1</span>] + a[<span class="number">2</span>] + a[<span class="number">3</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="使用CUDA的注意事项"><a href="#使用CUDA的注意事项" class="headerlink" title="使用CUDA的注意事项"></a>使用CUDA的注意事项</h2><p>几个原则需要尽可能保证</p><ol><li>保持SM尽可能忙碌<ol><li>加大数据量或者减小线程块大小</li></ol></li><li>优化存储器的使用<ol><li>全局存储器合并访问</li><li>使用constant或shared memory</li></ol></li><li>对齐分配空间<ol><li>关于<a href="https://www.ibm.com/developerworks/library/pa-dalign/" target="_blank" rel="noopener">为什么</a></li></ol></li></ol><h2 id="CUDA-naive的优化"><a href="#CUDA-naive的优化" class="headerlink" title="CUDA naive的优化"></a>CUDA naive的优化</h2><p>关键在于：理解这里为什么要使用转置矩阵来达到合并访存的目的</p><h3 id="第一步-cuda-naive"><a href="#第一步-cuda-naive" class="headerlink" title="第一步-cuda naive"></a>第一步-cuda naive</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __<span class="function">global__ <span class="title">mxvNaive</span><span class="params">(<span class="keyword">int</span> rowSize, <span class="keyword">int</span> columnSize, <span class="keyword">int</span> columnPitch, <span class="keyword">const</span> <span class="keyword">float</span> *d_matrix, <span class="keyword">const</span> <span class="keyword">float</span> *d_vec, <span class="keyword">float</span> *d_r)</span></span>&#123;</span><br><span class="line">    uint id = blockDim.x*blockIdx.x+ threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span>(rowSize&lt;= id) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">float</span> temp = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i= <span class="number">0</span>; i&lt; columnSize; i++)&#123;</span><br><span class="line">    temp += d_matrix[id*columnPitch+i]*d_vec[i]; </span><br><span class="line">        <span class="comment">// d_matrix[id*columnPitch+i]这里存在不能合并访存的问题</span></span><br><span class="line">    &#125;</span><br><span class="line">    d_r[id] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="第二步-发现合并访存的问题"><a href="#第二步-发现合并访存的问题" class="headerlink" title="第二步-发现合并访存的问题"></a>第二步-发现合并访存的问题</h3><p><img src="https://lh3.googleusercontent.com/-hTGicVs9O-g/XD2gvBT9DHI/AAAAAAAANy0/OecalPhUJ9Al47t8fZrVLgkejFgThupkQCHMYCw/s0/Acrobat_2019-01-15_16-58-36.png" alt=""></p><p>解决方法：先将矩阵转置，就可以达到合并访存的要求了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 转置后的for循环</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i= <span class="number">0</span>; i&lt; rowSize; i++)&#123;</span><br><span class="line">temp += d_matrix[i*columnPitch+id]*d_vec[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="访存优化"><a href="#访存优化" class="headerlink" title="访存优化"></a>访存优化</h2><p>关注到<code>d_vec</code>数组是不会改变的，因此考虑使用constant或者shared memory进行优化</p><h3 id="使用constant-memory优化"><a href="#使用constant-memory优化" class="headerlink" title="使用constant memory优化"></a>使用constant memory优化</h3><ol><li><p>将一整个向量都放进constant memory中，得到以下代码：</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __<span class="function">global__ <span class="title">mxvNaiveTransposeConstant</span><span class="params">(<span class="keyword">int</span> rowSize, <span class="keyword">int</span> columnSize, <span class="keyword">int</span> columnPitch, <span class="keyword">const</span> <span class="keyword">float</span> *d_matrix, <span class="keyword">const</span> <span class="keyword">int</span> start, <span class="keyword">float</span> *d_r)</span></span>&#123;</span><br><span class="line">    uint id = blockDim.x*blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span>(columnSize &lt;= id) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">float</span> temp = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">int</span> end = start+CONSTANTSIZE &gt; rowSize ? rowSize : start+CONSTANTSIZE;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = start; i &lt; end; i++)&#123;</span><br><span class="line">    temp += d_matrix[i*columnPitch+id]*c_v[i-start];</span><br><span class="line">    &#125;</span><br><span class="line">d_r[id] += temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>如果向量超过了constant memory的上限，那就</p><ol><li>分批，多次传输，启动内核。</li></ol></li></ol><h3 id="使用shared-memory优化"><a href="#使用shared-memory优化" class="headerlink" title="使用shared memory优化"></a>使用shared memory优化</h3><ol><li><p>关注到：可以在一个block内共享向量$v$，考虑使用shared memory</p></li><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __<span class="function">global__ <span class="title">mxvNaiveTransposeShared</span><span class="params">(<span class="keyword">int</span> rowSize, <span class="keyword">int</span> columnSize, <span class="keyword">int</span> columnPitch, <span class="keyword">const</span> <span class="keyword">float</span> *d_matrix, <span class="keyword">const</span> <span class="keyword">float</span> *d_v, <span class="keyword">const</span> <span class="keyword">int</span> sharedSize, <span class="keyword">float</span> *d_r)</span></span>&#123;</span><br><span class="line">    uint id = blockDim.x*blockIdx.x+ threadIdx.x;</span><br><span class="line">    <span class="keyword">float</span> temp = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="keyword">float</span> s_v[];</span><br><span class="line">    <span class="comment">// 外层循环，每次加载一段大小为sharedsize的向量v进行计算</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> start = <span class="number">0</span>; start &lt; rowSize; start += sharedSize)&#123;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll 4</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i= threadIdx.x; i&lt; sharedSize&amp;&amp;i+start&lt;rowSize; i+= blockDim.x)&#123;</span><br><span class="line">        s_v[i] = d_v[start+i]; <span class="comment">// 关键在于这里加载shared memory</span></span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        <span class="keyword">if</span>(columnSize&lt;= id) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">int</span> end = start+sharedSize&gt; rowSize? rowSize: start+sharedSize;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll 8</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = start; i &lt; end; i++)&#123;</span><br><span class="line">            <span class="comment">// 使用shared memory 访存得到极大提升</span></span><br><span class="line">        temp += d_matrix[i*columnPitch+id]*s_v[i-start];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(id &lt; columnSize)</span><br><span class="line">    d_r[id] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="block模式与warp模式"><a href="#block模式与warp模式" class="headerlink" title="block模式与warp模式"></a>block模式与warp模式</h2><p>问题：如果不转置矩阵如何计算？</p><h3 id="block模式"><a href="#block模式" class="headerlink" title="block模式"></a>block模式</h3><p>关键：一个block处理矩阵的一行和向量乘积，其中block中的每个线程处理该行中的一个与对应向量元素的乘积,然后归约。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __<span class="function">global__ <span class="title">mxvBlock</span><span class="params">(<span class="keyword">int</span> rowSize, <span class="keyword">int</span> columnSize, <span class="keyword">int</span> pitchItem, <span class="keyword">const</span> <span class="keyword">float</span>* __restrict__ d_matrix,constfloat* __restrict__ d_vec, <span class="keyword">float</span>* __restrict__ d_r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid= threadIdx.x;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="keyword">float</span> s_r[];</span><br><span class="line">    <span class="keyword">float</span> temp = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i= tid; i&lt; columnSize; i+= blockDim.x)&#123;</span><br><span class="line">    temp += d_matrix[blockIdx.x*pitchItem+i]*d_vec[i];</span><br><span class="line">    &#125;</span><br><span class="line">    s_r[tid] = temp; __syncthreads();</span><br><span class="line">    ……<span class="comment">//省略归约代码</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="warp模式"><a href="#warp模式" class="headerlink" title="warp模式"></a>warp模式</h3><p>具体的计算和block模式差不多,只是使用一个warp线程计算矩阵的一行与向量的乘积,在我的测试中发现,这个算法对于行数大于列数的矩阵效果很好,很多时候性能是block的两倍以上。</p><h2 id="使用cuBlas包"><a href="#使用cuBlas包" class="headerlink" title="使用cuBlas包"></a>使用cuBlas包</h2><p>成为调包侠：</p><p>函数: cublasSgemv——cuBlas包的矩阵向量乘法</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后的优化结果可见</p><p><img src="https://lh3.googleusercontent.com/-eO3K2LRIf3Q/XD2cyoaQgII/AAAAAAAANyo/CwokcPDNYRkX3mZo8UqcINMGXtAgpmxOwCHMYCw/s0/Acrobat_2019-01-15_16-41-45.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;HPC复习7-2-矩阵向量乘法优化&quot;&gt;&lt;a href=&quot;#HPC复
      
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习7.1-cuda矩阵乘法优化</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A07-1-cuda%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习7-1-cuda矩阵乘法优化/</id>
    <published>2019-01-15T11:58:43.000Z</published>
    <updated>2019-01-15T12:02:39.602Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习7-1-矩阵乘法优化"><a href="#HPC复习7-1-矩阵乘法优化" class="headerlink" title="HPC复习7.1-矩阵乘法优化"></a>HPC复习7.1-矩阵乘法优化</h1><h2 id="naive实现"><a href="#naive实现" class="headerlink" title="naive实现"></a>naive实现</h2><p>具体思路就很简单了，仅使用一个块，然后块中每一个线程计算矩阵的一个元素。</p><p>算法图示可见<img src="https://lh3.googleusercontent.com/-hk3rnSa5zm4/XD2a5NgdiSI/AAAAAAAANyc/HDkg-4FY4icQgSXBtHLzhZ7XQ8PliUd3gCHMYCw/s0/Snipaste_2019-01-15_16-33-36.png" alt=""></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 矩阵乘法的内核函数——每个线程都要执行的代码</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">MatrixMulKernel</span><span class="params">(<span class="keyword">float</span>* Md, <span class="keyword">float</span>* Nd, <span class="keyword">float</span>* Pd, intWidth)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 2维的线程ID号</span></span><br><span class="line">    inttx= threadIdx.x;</span><br><span class="line">    intty= threadIdx.y;</span><br><span class="line">    <span class="comment">// Pvalue用来保存被每个线程计算完成后的矩阵的元素</span></span><br><span class="line">    <span class="keyword">float</span> Pvalue= <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//每个线程计算一个元素</span></span><br><span class="line">    <span class="keyword">for</span> (intk = <span class="number">0</span>; k &lt; Width; ++k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">float</span> Melement= Md[ty* Width + k];</span><br><span class="line">        <span class="keyword">float</span> Nelement= Nd[k * Width + tx];</span><br><span class="line">        Pvalue += Melement* Nelement;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将计算结果写入设备存储器中</span></span><br><span class="line">    Pd[ty* Width + tx] = Pvalue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>这样的实现是有问题的：</p><ol><li>计算的时间与访存时间比例相当，受存储器延迟的影响很大。</li><li>矩阵的大小收到线程块所能容纳的最大线程数的限制。</li></ol></blockquote><h2 id="处理任意大小的方形矩阵"><a href="#处理任意大小的方形矩阵" class="headerlink" title="处理任意大小的方形矩阵"></a>处理任意大小的方形矩阵</h2><p>解决了在上面的实现中，无法处理任意大小的方形矩阵的问题。</p><p>关键思想：</p><ol><li>将矩阵分块，每一个线程块block计算其中的一个子矩阵</li><li>如果矩阵块的数量大于最大的上限时，需要在内核附近设置一个循环（一个已经用过的技巧，我会的啦）</li></ol><blockquote><p>问题：每一个线程都要访问global memory获取矩阵的一整行和一整列元素</p><p>访存带宽成为了计算的瓶颈</p></blockquote><h2 id="分片矩阵乘法"><a href="#分片矩阵乘法" class="headerlink" title="分片矩阵乘法"></a>分片矩阵乘法</h2><p>使用高带宽的片上存储器”shared memory”缓解了访存瓶颈</p><p>关键思想：重用数据，原来的矩阵乘法实现中，每一个区域都有多个线程多次访问，如果数据可以重用，可以大大降低计算所需的带宽。</p><p>分片矩阵乘法-算法关键：</p><ol><li>代价估计：<code>浮点操作：全局存储器读出操作＝16: 1</code>，说明访存代价占比不高</li><li><img src="figure/1545275255554.png" alt="1545275255554"></li><li>算法细节<ol><li>逐个子矩阵块，依次计算，求和到结果矩阵上</li><li>进行子矩阵块的运算前，先将子矩阵块复制到<code>shared memory</code>上<ol><li>一个合并访存(访问global memory)的技巧：每一个线程读取一个值，然后使用同步原语保证同步</li></ol></li><li>每一个线程计算完子矩阵块的一个结果后，合并访存的技巧依然使用，写位于global memory的矩阵<ol><li>每一个线程仅写回自己计算的值，合并访存，然后使用同步原语</li></ol></li></ol></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每个线程块有TILE_WIDTH2个线程</span></span><br><span class="line"><span class="function">dim3 <span class="title">dimBlock</span><span class="params">(TILE_WIDTH, TILE_WIDTH)</span></span>;</span><br><span class="line"><span class="comment">//有(Width/TILE_WIDTH)2个线程块</span></span><br><span class="line"><span class="function">dim3 <span class="title">dimGrid</span><span class="params">(Width/TILE_WIDTH, Width/TILE_WIDTH)</span></span>;</span><br><span class="line"><span class="comment">//调用内核函数</span></span><br><span class="line">MatrixMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(Md, Nd, Pd，Width);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下是内核函数</span></span><br><span class="line"><span class="comment">// part 1 将数据从global memory加载到shared memory上</span></span><br><span class="line"><span class="comment">//获得线程块号</span></span><br><span class="line">intbx= blockIdx.x;</span><br><span class="line">intby = blockIdx.y;</span><br><span class="line"><span class="comment">//获得块内的线程号</span></span><br><span class="line">inttx= threadIdx.x;</span><br><span class="line">intty= threadIdx.y;</span><br><span class="line"><span class="comment">//Pvalue：线程计算完成后的子矩阵元素——自动变量</span></span><br><span class="line"><span class="keyword">float</span> Pvalue= <span class="number">0</span>;</span><br><span class="line"><span class="comment">//循环，遍历M和N的所有子矩阵</span></span><br><span class="line"><span class="keyword">for</span> (intm = <span class="number">0</span>; m &lt; Width/TILE_WIDTH; ++m) &#123;</span><br><span class="line">    <span class="comment">// 获取指向当前矩阵M子矩阵的指针Msub</span></span><br><span class="line">    Float* Mdsub= GetSubMatrix(Md, m, by, Width);</span><br><span class="line">    <span class="comment">//获取指向当前矩阵N的子矩阵的指针Nsub</span></span><br><span class="line">    Float* Ndsub= GetSubMatrix(Nd, bx, m, Width);</span><br><span class="line">    <span class="comment">//共享存储器空间声明</span></span><br><span class="line">    __shared__floatMds[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    __shared__floatNds[TILE_WIDTH][TILE_WIDTH];</span><br><span class="line">    <span class="comment">// 每个线程载入M的子矩阵的一个元素</span></span><br><span class="line">    Mds[ty][tx] = GetMatrixElement(Mdsub, tx, ty);</span><br><span class="line">    <span class="comment">//每个线程载入N的子矩阵的一个元素</span></span><br><span class="line">    Nds[ty][tx] = GetMatrixElement(Ndsub, tx, ty);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// part 2 每个线程计算一个值结果</span></span><br><span class="line"><span class="comment">//同步，在计算之前，确保子矩阵所有的元素都已载入共享存储器中</span></span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="comment">//每个线程计算线程块内子矩阵中的一个元素</span></span><br><span class="line"><span class="keyword">for</span> (intk = <span class="number">0</span>; k &lt; TILE_WIDTH; ++k)</span><br><span class="line">Pvalue+= Mds[ty][k] * Nds[k][tx];</span><br><span class="line"><span class="comment">//同步，确保重新载入新的M和N子矩阵数据前，上述计算操作已全部完成</span></span><br><span class="line">__syncthreads();</span><br><span class="line"></span><br><span class="line"><span class="comment">// part 3 合并访存 写回</span></span><br><span class="line"><span class="comment">// 获取指向矩阵P的子矩阵的指针</span></span><br><span class="line">Matrix Psub= GetSubMatrix(P, bx, by);</span><br><span class="line"><span class="comment">//向全局存储器写入线程块计算后的结果子矩阵</span></span><br><span class="line"><span class="comment">//每个线程写入一个元素</span></span><br><span class="line">SetMatrixElement(Psub, tx, ty, Pvalue);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;HPC复习7-1-矩阵乘法优化&quot;&gt;&lt;a href=&quot;#HPC复习7
      
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习6-CUDA优化</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A06-CUDA%E4%BC%98%E5%8C%96/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习6-CUDA优化/</id>
    <published>2019-01-15T08:13:09.000Z</published>
    <updated>2019-01-15T08:13:49.733Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习6-cuda优化"><a href="#HPC复习6-cuda优化" class="headerlink" title="HPC复习6-cuda优化"></a>HPC复习6-cuda优化</h1><p>在cuda程序的优化中，需要考虑以下几个问题</p><ol><li>了解SM核中所提供的资源，并合理分配。</li><li>确定kernel的启动参数，以尽可能提高cuda程序的性能。</li><li>理解warp隐藏延时的作用，知道为什么warp越多越能隐藏延时。</li><li>通过数据预读取隐藏访存延时。</li><li>了解不同指令的吞吐量，并优化之。</li></ol><a id="more"></a><h2 id="SM资源分割"><a href="#SM资源分割" class="headerlink" title="SM资源分割"></a>SM资源分割</h2><h3 id="需要确定的值"><a href="#需要确定的值" class="headerlink" title="需要确定的值"></a>需要确定的值</h3><ol><li>block的数量</li><li>thread的数量</li></ol><h3 id="与分配资源有关的参数"><a href="#与分配资源有关的参数" class="headerlink" title="与分配资源有关的参数"></a>与分配资源有关的参数</h3><ol><li>线程块槽数量（thread block slot）：block数量的上限</li><li>线程槽数量（thread slot）</li><li>寄存器的数量</li><li>shared memory的大小</li></ol><h3 id="分配资源的数量，与哪些因素有关"><a href="#分配资源的数量，与哪些因素有关" class="headerlink" title="分配资源的数量，与哪些因素有关"></a>分配资源的数量，与哪些因素有关</h3><ol><li>原则一：单个SM核上分配的线程数（block*每个block具有的线程数）越大，线程级别的并行越大（前提与sm核实际运行的状况有关，需要想清楚）<ol><li>线程越多，warp越多，可运行的用来隐藏访存时间的warp就越多，因此就可以尽可能让GPU达到满负荷工作，不会由于访存延迟使得gpu空闲。</li></ol></li><li>原则二：单个SM核上的所有寄存器，平均分配给各个线程，因此，线程数量越多，单个线程可用的寄存器越少<ol><li>性能悬崖警惕：减少1/3的线程（并行度），仅仅让每一个线程增加了1个寄存器，除非由于寄存器不足导致了较大的访存开销，否则小心调整。</li></ol></li><li>原则三：对block的数量需要注意，尽量让每一个SM核上都具有多个block，这样子如果一个block在等待同步，可以启动另一个block<ol><li>需要知道SM核的数量</li></ol></li><li>原则五：『关于shared memory』<ol><li>shared memory按block分割，block过多可能会让单个block所具有的shared memory脱销</li></ol></li></ol><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p>对G80而言，与分配相关的参数可见：</p><p><img src="https://lh3.googleusercontent.com/-kTJQHo2890Y/XD2P6Ak0g2I/AAAAAAAANx4/HmgaxgQefeAXjfCrvVLuaHq1WeG4QGtAwCHMYCw/s0/Acrobat_2019-01-15_15-46-48.png" alt=""></p><p>有这样的一个情景，在每个block都含256个线程的情况下，一个SM核可能有以下两种情况：</p><p><img src="https://lh3.googleusercontent.com/-lbe-eNMWhP0/XD2PuXTO_aI/AAAAAAAANx0/Z0Grw8p5d0sLkhYhQM5eEafxGO_6aMF8ACHMYCw/s0/Acrobat_2019-01-15_15-46-01.png" alt=""></p><p>这里就可以发现，每个线程多了一个寄存器，但是带来的影响是，总的可以运行的线程的数量变为原来的2/3，也就是说，并行度是原来的2/3了。</p><p>这个可以给我一个启示：除非为了隐藏global memory访存延迟，否则尽可能不要为了增加寄存器数量而降低并行性。</p><h2 id="Kernel启动参数配置"><a href="#Kernel启动参数配置" class="headerlink" title="Kernel启动参数配置"></a>Kernel启动参数配置</h2><h3 id="需要确定以下参数"><a href="#需要确定以下参数" class="headerlink" title="需要确定以下参数"></a>需要确定以下参数</h3><ol><li><p>grid（block的数量）：主要看$\frac{blocks}{sm}$</p><ol><li>大于1：每个SM至少有一个block在执行</li><li>大于2：多个block可以在SM核上并发执行，如果一个block在等待同步，可以启动另一个block</li><li>大于100：对未来设备有很好的伸缩性</li></ol></li><li><p>block（thread的数量）</p><ol><li><p>块大小必须为32的倍数</p></li><li><p>warp尽可能多，隐藏延时</p></li><li><p>64,128,256 等等，试一下，经验成分比较多</p></li></ol></li></ol><h2 id="隐藏延时-相关计算"><a href="#隐藏延时-相关计算" class="headerlink" title="隐藏延时-相关计算"></a>隐藏延时-相关计算</h2><p>问题：需要使用多少个warp来隐藏某操作的延时，此时占用率多大？</p><blockquote><p>占用率：激活warp与最大可容纳warp数目的比值</p><p>最大warp数目: 32 in Tesla, 48 in Fermi</p></blockquote><h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><p><img src="https://lh3.googleusercontent.com/-_DlX9mCbLX8/XD2Sz8Y7WRI/AAAAAAAANyI/clacnlWedsAqyV5mozO0HiLdBlBAskVVwCHMYCw/s0/Acrobat_2019-01-15_15-59-11.png" alt=""></p><h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><p><img src="https://lh3.googleusercontent.com/-pgQYXjs0hrU/XD2S4yZQCiI/AAAAAAAANyM/rwa9BCsy02EMvIOUSQZmUg4dURBSj05LACHMYCw/s0/Acrobat_2019-01-15_15-59-31.png" alt=""></p><h2 id="数据预读"><a href="#数据预读" class="headerlink" title="数据预读"></a>数据预读</h2><blockquote><p>数据预读：在某global memroy变量的读操作，与该变量的实际使用语句之间，插入与该数据无关的独立指令，可以隐藏访存延迟</p></blockquote><p>例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> m = Md[i];</span><br><span class="line"><span class="keyword">float</span> f = a*b + c*d; <span class="comment">// 无关指令，隐藏访存延时</span></span><br><span class="line"><span class="keyword">float</span> f2 = m * f;</span><br></pre></td></tr></table></figure><h3 id="如何在矩阵乘法中使用预读操作进行优化"><a href="#如何在矩阵乘法中使用预读操作进行优化" class="headerlink" title="如何在矩阵乘法中使用预读操作进行优化"></a>如何在矩阵乘法中使用预读操作进行优化</h3><p>代码模板如下，</p><p>注意到中间的预读在计算点积前面，这样子中间的点积在运算时便可以隐藏预读内存产生的误差了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Load first tile into registers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="comment">/* ... */</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Deposit registers into shared memory</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Load next tile into registers</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Accumulate dot product</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="指令吞吐量优化"><a href="#指令吞吐量优化" class="headerlink" title="指令吞吐量优化"></a>指令吞吐量优化</h2><p>了解每一种指令的吞吐量，减少使用昂贵的指令。</p><h3 id="一些指令的吞吐量"><a href="#一些指令的吞吐量" class="headerlink" title="一些指令的吞吐量"></a>一些指令的吞吐量</h3><table><thead><tr><th>int &amp; fp32</th><th>2 cycles</th></tr></thead><tbody><tr><td>fp64:</td><td>2 cycles</td></tr><tr><td>fp32 transendental</td><td>8 cycles</td></tr><tr><td>int devide/modulo</td><td>expensive</td></tr></tbody></table><blockquote><p>优化建议：</p><ol><li>与<code>2^n</code>运算，尽量使用位运算，如<code>&gt;&gt; n``&lt;&lt; n</code></li><li>在float常量中添加f，（缺省是double，会导致隐式的类型转换）</li></ol></blockquote><h3 id="数学函数的吞吐量提高"><a href="#数学函数的吞吐量提高" class="headerlink" title="数学函数的吞吐量提高"></a>数学函数的吞吐量提高</h3><ol><li>cuda中两种类型的运行时数学函数<ol><li>func()</li><li>__func()：使用硬件加速，SFU，精度低</li></ol></li><li><code>--use-fast-math</code>编译指令，强制使用硬件加速的数学函数</li></ol><h3 id="循环展开"><a href="#循环展开" class="headerlink" title="循环展开"></a>循环展开</h3><blockquote><p>原理：循环中除了循环体，还有更新计数器，判断条件，计算地址等指令，减少这些无关指令的运算可以加速</p></blockquote><p>例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k =<span class="number">0</span>;k &lt; <span class="number">16</span>;++k)</span><br><span class="line">&#123;</span><br><span class="line">Pvalue +=Ms[ty][k]*Ns[k][tx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这其中含有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2条浮点运算</span><br><span class="line">1条循环分支</span><br><span class="line">2条地址运算</span><br><span class="line">1条循环计数器自增</span><br></pre></td></tr></table></figure><p><strong>仅1/3是浮点计算！！！</strong></p><p>做以下改动：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pvalue += </span><br><span class="line">    Ms[ty][<span class="number">0</span>] * Ns[<span class="number">0</span>][tx] + </span><br><span class="line">    Ms[ty][<span class="number">1</span>] * Ns[<span class="number">1</span>][tx] + </span><br><span class="line">    ...</span><br><span class="line">    Ms[ty][<span class="number">15</span>] * Ns[<span class="number">15</span>][tx];</span><br></pre></td></tr></table></figure><p>从而减少了循环分支，自增，同时地址运算也可以减少。</p><p>自动完成循环展开：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll 16</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> k =<span class="number">0</span>;k &lt; <span class="number">16</span>;++k)</span><br><span class="line">&#123;</span><br><span class="line">Pvalue +=Ms[ty][k]*Ns[k][tx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>由于展开能够消除分支以及一些管理归纳变量的代码，因此可以摊销一些分支开销。<br>展开可以积极调度（或管道化）循环以掩盖一些延迟。如果有足够的空闲寄存器使变量保持活动状态，因为通过展开相关性链展露了关键路径，这将非常有用。</p></blockquote><p>但</p><blockquote><p>展开过度或展开非常大的循环时，可能导致代码篇幅增加。如果展开后的循环不能再放入跟踪缓存 (TC)，这将有害无益。</p><p>展开循环体中包含分支的循环时，会增加对 BTB 容量的需求。如果展开后循环的迭代次数是 16 或更少，则分支预测应该能正确预测循环体中改变方向的分支。</p></blockquote><h2 id="课上提醒"><a href="#课上提醒" class="headerlink" title="课上提醒"></a>课上提醒</h2><ol><li>如果一整个warp没有指令要执行，不会占住SP，一上来就下去，但只要warp中有一个线程要执行，那就一定会占住SP</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习6-cuda优化&quot;&gt;&lt;a href=&quot;#HPC复习6-cuda优化&quot; class=&quot;headerlink&quot; title=&quot;HPC复习6-cuda优化&quot;&gt;&lt;/a&gt;HPC复习6-cuda优化&lt;/h1&gt;&lt;p&gt;在cuda程序的优化中，需要考虑以下几个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;了解SM核中所提供的资源，并合理分配。&lt;/li&gt;
&lt;li&gt;确定kernel的启动参数，以尽可能提高cuda程序的性能。&lt;/li&gt;
&lt;li&gt;理解warp隐藏延时的作用，知道为什么warp越多越能隐藏延时。&lt;/li&gt;
&lt;li&gt;通过数据预读取隐藏访存延时。&lt;/li&gt;
&lt;li&gt;了解不同指令的吞吐量，并优化之。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习5.3-CUDA线程</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A05-3-CUDA%E7%BA%BF%E7%A8%8B/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习5-3-CUDA线程/</id>
    <published>2019-01-15T07:21:22.000Z</published>
    <updated>2019-01-15T07:21:49.107Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><hr><h2 id="typora-copy-images-to-figure"><a href="#typora-copy-images-to-figure" class="headerlink" title="typora-copy-images-to: figure"></a>typora-copy-images-to: figure</h2><h1 id="HPC复习5-3-CUDA线程"><a href="#HPC复习5-3-CUDA线程" class="headerlink" title="HPC复习5.3-CUDA线程"></a>HPC复习5.3-CUDA线程</h1><p>这里主要想想明白一个事：</p><p>已知的是，会有多个block，分配到SM核上来执行。</p><p>也知道，单一时间上，SM核仅有一个warp在运行。</p><p>问题是：</p><ol><li>SM核内部的结构是如何的？</li><li>warp的实际执行情况是如何的？</li><li>warp的并发执行是否会引发一些问题？如何解决？</li></ol><a id="more"></a><h2 id="SM核架构"><a href="#SM核架构" class="headerlink" title="SM核架构"></a>SM核架构</h2><p>对SM核的架构该如何理解？</p><ol><li>2个warp调度器与2个指令分派单元能够将2个warp同时进行发射和执行:</li><li>双warp调度器先选择两个warp，然后从每个warp发射一条指令到一个十六核心的组，或是十六个读写单元或是四个SFU。</li></ol><p><img src="figure/Acrobat_2019-01-15_15-18-20.png" alt=""></p><h2 id="Single-Warp"><a href="#Single-Warp" class="headerlink" title="Single Warp"></a>Single Warp</h2><p>对于单个warp内部的并行执行，可能会遇到以下的一些问题：</p><h3 id="单个warp上可能发生的访存冲突"><a href="#单个warp上可能发生的访存冲突" class="headerlink" title="单个warp上可能发生的访存冲突"></a>单个warp上可能发生的访存冲突</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (warpIdx == <span class="number">0</span>) &#123;</span><br><span class="line">    __shared__ <span class="keyword">int</span> v;</span><br><span class="line">    v = <span class="number">0</span>;</span><br><span class="line">    ++v;</span><br><span class="line">    v == ?</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>注意到share memory在一个block内是能够共享的，因此这一段代码由于会导致错误。</li><li>解决这一个问题可以使用<code>atomicAdd(&amp;v, 1);</code>来实现。</li><li>注意到CUDA不支持临界区。</li></ol><p><img src="https://lh3.googleusercontent.com/-nSO5ewSSqtw/XD15aShcuWI/AAAAAAAANvw/b4Vgwb5THG0NvN8vTR011C3KATyKXZ1IgCHMYCw/s0/Acrobat_2019-01-15_14-10-49.png" alt=""></p><h3 id="warp如何处理分支指令？"><a href="#warp如何处理分支指令？" class="headerlink" title="warp如何处理分支指令？"></a>warp如何处理分支指令？</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> warpIdx = threadIdx.x / <span class="number">32</span>;</span><br><span class="line"><span class="keyword">int</span> laneIdx = threadIdx.x % <span class="number">32</span>;</span><br><span class="line"><span class="keyword">if</span> (warpIdx == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (laneIdx % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">    doA();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    doB();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>事实上，warp线程在执行上面代码的时候，图示可见如下：</p><p><img src="https://lh3.googleusercontent.com/-iMVFZM74Hxk/XD16kGp5FmI/AAAAAAAANv4/A4cKphyGtmAWbN00g02kvXHckXrIUbW8ACHMYCw/s0/Acrobat_2019-01-15_14-15-43.png" alt=""></p><ol><li>有很多branch的代码是低效的。</li><li>复杂的控制流，如break，continue，会导致代码低效，可能会出现bug。</li></ol><h3 id="Warp-functions"><a href="#Warp-functions" class="headerlink" title="Warp functions"></a>Warp functions</h3><ol><li><code>__all(predicate);</code><ol><li>return 1 if and only if predicate evaluates to non-zero for all of them.</li></ol></li><li><code>__any(predicate);</code><ol><li>return 1 if and only if predicate evaluates to non-zero for any of them.</li></ol></li><li><img src="https://lh3.googleusercontent.com/-vw2oRXY0li8/XD176bbVsSI/AAAAAAAANwE/YbZ0jpSYt-sFHT40WYokkHGAJeXx60SkACHMYCw/s0/Acrobat_2019-01-15_14-21-29.png" alt=""></li><li><code>__ballot(predicate);</code><ol><li>return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp and the Nth thread is active.</li><li><img src="https://lh3.googleusercontent.com/-ueHU6gtByjg/XD18A5HFrwI/AAAAAAAANwI/p8mfYDejS8ch_4Uv6nWAmGBPhbzYTmPawCHMYCw/s0/Acrobat_2019-01-15_14-21-56.png" alt=""></li></ol></li></ol><h2 id="Multi-Warp"><a href="#Multi-Warp" class="headerlink" title="Multi Warp"></a>Multi Warp</h2><p>多个warp并发执行，可能会导致一些问题。</p><ol><li>多个warp之间的并发执行顺序不定，这会带来一些问题（联想：多线程可能会带来的问题）</li><li>warp内分支也可能会带来问题，导致性能损失。</li></ol><h3 id="warp之间无序的并发执行会带来哪些问题？"><a href="#warp之间无序的并发执行会带来哪些问题？" class="headerlink" title="warp之间无序的并发执行会带来哪些问题？"></a>warp之间无序的并发执行会带来哪些问题？</h3><ol><li>RaW（read after write）<ol><li>如果先写后读，由于后面读的时候，不知道其他warp写了没，可能会导致问题。</li><li><img src="https://lh3.googleusercontent.com/-uZj9SMP3wek/XD2FvMI87DI/AAAAAAAANwY/Q9uTD9MADB8MWDfVfsvAa2TUKIudidUWQCHMYCw/s0/Acrobat_2019-01-15_15-03-24.png" alt=""></li><li><img src="https://lh3.googleusercontent.com/-yt2kMUSQDq0/XD2FySx5pXI/AAAAAAAANwc/LeaX4scfJho6PhzfKyFJdsdAx5BbvfUrwCHMYCw/s0/Acrobat_2019-01-15_15-03-38.png" alt=""></li></ol></li><li>WaR（write after read）<ol><li>读后写，同样的写的时候，不知道前面读的值是否是最新的</li><li><img src="https://lh3.googleusercontent.com/-VJn672ZJxe8/XD2F4RJLBHI/AAAAAAAANwg/9y8nqyw1vE4Mge5Z4juJzuFSQK_7nu1FwCHMYCw/s0/Acrobat_2019-01-15_15-04-01.png" alt=""></li><li><img src="https://lh3.googleusercontent.com/-iV8FnRM_3eU/XD2F8DrVM9I/AAAAAAAANwk/Ym3R5o17uIMJglDNqYgX1uN--2SU0ikaQCHMYCw/s0/Acrobat_2019-01-15_15-04-17.png" alt=""></li></ol></li><li>WaW（write after write）<ol><li>写后写，同样的，不同的warp之间的写语句，可能运行的相对顺序不一样。</li><li><img src="https://lh3.googleusercontent.com/-yOpBk_JMEj4/XD2GCgXNvkI/AAAAAAAANws/C2i1Fn7EUSga1XeiHh0QbVy7RsPI_ck2ACHMYCw/s0/Acrobat_2019-01-15_15-04-42.png" alt=""></li><li><img src="https://lh3.googleusercontent.com/-b3Q1lYpBoVY/XD2GHe4prVI/AAAAAAAANww/iU36UTs-t5ARlM2uFWfZGrQFYgP2SCl2wCHMYCw/s0/Acrobat_2019-01-15_15-05-01.png" alt=""></li></ol></li><li>如何解决：使用<code>__syncthreads()</code>函数<ol><li><img src="https://lh3.googleusercontent.com/-iEnKp2ahX90/XD2GqKA7g6I/AAAAAAAANxA/1TkcMbml8LkK_9l9UVX-HesqNxjL-3O5gCHMYCw/s0/Acrobat_2019-01-15_15-07-20.png" alt=""></li></ol></li></ol><h3 id="branch如何影响多个warp运行的性能？"><a href="#branch如何影响多个warp运行的性能？" class="headerlink" title="branch如何影响多个warp运行的性能？"></a>branch如何影响多个warp运行的性能？</h3><p>对代码段：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (laneIdx % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">doA();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">doB();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li><p>结论：</p><ol><li>如果一个warp内的线程有的运行<code>doA</code>，有的运行<code>doB</code>，那么这一个warp必须两部分都运行</li><li>如果一个warp内的线程仅运行其中一个函数，那么该分支语句就不会带来影响</li></ol></li><li><p>以下分三种情况分别观察情况</p><ol><li>（warp-divergent）一个warp内，有的需要运行<code>doA</code>，有的需要运行<code>doB</code><ol><li><img src="https://lh3.googleusercontent.com/-G68TmtaLlMA/XD2Hc6NgCPI/AAAAAAAANxM/VwoZls0KjzMr-s9XCHH451eYMx3XBRKVQCHMYCw/s0/Acrobat_2019-01-15_15-10-43.png" alt=""></li></ol></li><li>(warp-uniform)在同一个block内，有的warp运行<code>doA</code>，有的warp运行<code>doB</code><ol><li><img src="https://lh3.googleusercontent.com/-1kdv3JnV8eg/XD2Hj5fN6JI/AAAAAAAANxQ/pLe-5ghYggQ70bhXCeU5ips2MG02p_8WwCHMYCw/s0/Acrobat_2019-01-15_15-11-11.png" alt=""></li></ol></li><li>(block-uniform)不同的block，有的block运行<code>doA</code>，有的block运行<code>doB</code><ol><li><img src="https://lh3.googleusercontent.com/-441QzpylRK0/XD2HowdmwLI/AAAAAAAANxU/0nkBL7fTwycSth6o5VV7swq4K9JT_RXUQCHMYCw/s0/Acrobat_2019-01-15_15-11-32.png" alt=""></li></ol></li><li>总结：block-uniform方式能够更好的减少性能损失</li></ol></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这幅图我觉得很好地将之前讲过的很多东西都联系了在一起，那就放上来慢慢观赏吧。</p><p><img src="https://lh3.googleusercontent.com/-8wO_TbAmW08/XD2IhLEAcTI/AAAAAAAANxg/vga83BRrlc4Nk8ebfYTnoOy1FLbJIA6zACHMYCw/s0/Acrobat_2019-01-15_15-15-16.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h2 id=&quot;typora-copy-images-to-figure&quot;&gt;&lt;a href=&quot;#typora-copy-images-to-figure&quot; class=&quot;headerlink&quot; title=&quot;typora-copy-images-to: figure&quot;&gt;&lt;/a&gt;typora-copy-images-to: figure&lt;/h2&gt;&lt;h1 id=&quot;HPC复习5-3-CUDA线程&quot;&gt;&lt;a href=&quot;#HPC复习5-3-CUDA线程&quot; class=&quot;headerlink&quot; title=&quot;HPC复习5.3-CUDA线程&quot;&gt;&lt;/a&gt;HPC复习5.3-CUDA线程&lt;/h1&gt;&lt;p&gt;这里主要想想明白一个事：&lt;/p&gt;
&lt;p&gt;已知的是，会有多个block，分配到SM核上来执行。&lt;/p&gt;
&lt;p&gt;也知道，单一时间上，SM核仅有一个warp在运行。&lt;/p&gt;
&lt;p&gt;问题是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SM核内部的结构是如何的？&lt;/li&gt;
&lt;li&gt;warp的实际执行情况是如何的？&lt;/li&gt;
&lt;li&gt;warp的并发执行是否会引发一些问题？如何解决？&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习5.2-CUDA访存</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A05-2-CUDA%E8%AE%BF%E5%AD%98/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习5-2-CUDA访存/</id>
    <published>2019-01-15T05:30:18.000Z</published>
    <updated>2019-01-15T05:33:09.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习5-2-CUDA访存"><a href="#HPC复习5-2-CUDA访存" class="headerlink" title="HPC复习5.2-CUDA访存"></a>HPC复习5.2-CUDA访存</h1><p>这里就主要对cuda中的访存模式进行比较详细地说明吧。</p><ol><li>GPU中，5种不同存储部件的特性及使用方式</li><li>GPU中，如何使用合并访存加速</li><li>下图是GPU中的存储设备的大图</li></ol><p><img src="https://lh3.googleusercontent.com/-FME8Iu_B5cI/XD1D5vIEeaI/AAAAAAAANsE/9ObTV12e34ArBzh9o90oEjCsNBJ_X1LZQCHMYCw/s0/Acrobat_2019-01-15_10-22-29.png" alt=""></p><a id="more"></a><h2 id="GPU中的存储部件"><a href="#GPU中的存储部件" class="headerlink" title="GPU中的存储部件"></a>GPU中的存储部件</h2><h3 id="如何使用global-memory"><a href="#如何使用global-memory" class="headerlink" title="如何使用global memory?"></a>如何使用global memory?</h3><p>有两种方式：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="title">add4f</span><span class="params">(<span class="keyword">float</span>* u, <span class="keyword">float</span>* v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">    u[i]+=v[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> hostU[<span class="number">4</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="keyword">float</span> hostV[<span class="number">4</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="keyword">float</span>* devU, devV;</span><br><span class="line">    <span class="keyword">size_t</span> size = <span class="keyword">sizeof</span>(<span class="keyword">float</span>)*<span class="number">4</span>;</span><br><span class="line">    </span><br><span class="line">    cudaMalloc(&amp;devU, size);</span><br><span class="line">    cudaMalloc(&amp;devV, size);</span><br><span class="line">    </span><br><span class="line">    cudaMemcpy(devU, hostU, size, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(devV, hostV, size, cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    add4f&lt;&lt;&lt;<span class="number">1</span>,<span class="number">4</span>&gt;&gt;&gt;(devU, devV);</span><br><span class="line">    cudaMemcpy(hostU, devU, size, cudaMemcpyDeviceToHost);</span><br><span class="line">    </span><br><span class="line">    cudaFree(devV);</span><br><span class="line">    cudaFree(devU);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二种方式：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="keyword">float</span> devU[<span class="number">4</span>];</span><br><span class="line">__device__ <span class="keyword">float</span> devV[<span class="number">4</span>];</span><br><span class="line">__<span class="function">global__ <span class="title">addUV</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">    devU[i]+=devV[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> hostU[<span class="number">4</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="keyword">float</span> hostV[<span class="number">4</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="keyword">size_t</span> size = <span class="keyword">sizeof</span>(<span class="keyword">float</span>)*<span class="number">4</span>;</span><br><span class="line">    cudaMemcpyToSymbol(devU, hostU, size, <span class="number">0</span>, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpyToSymbol(devV, hostV, size, <span class="number">0</span>, cudaMemcpyHostToDevice);</span><br><span class="line">    addUV&lt;&lt;&lt;<span class="number">1</span>,<span class="number">4</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaMemcpyFromSymbol(hostU, devU, size, <span class="number">0</span>, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>一点点小疑问</p><p>cudaMemcpyToSymbol和cudaMemcpy的区别，可见<a href="https://blog.csdn.net/litdaguang/article/details/45047015" target="_blank" rel="noopener">link</a></p></blockquote><h3 id="如何使用constant-cache？该种存储空间有什么特点？"><a href="#如何使用constant-cache？该种存储空间有什么特点？" class="headerlink" title="如何使用constant cache？该种存储空间有什么特点？"></a>如何使用constant cache？该种存储空间有什么特点？</h3><ol><li><p>这样使用：</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__constant__ <span class="keyword">int</span> devVar</span><br><span class="line"></span><br><span class="line">cudaMemcpyToSymbol(</span><br><span class="line">    devVar, &amp;hostVar, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">0</span>,</span><br><span class="line">    cudaMemcpyHostToDevice</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">cudaMemcpyFromSymbol(</span><br><span class="line">    &amp;hostVar, devVar, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">0</span>,</span><br><span class="line">    cudaMemcpyDeviceToHost</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>注意到，这种类型的使用，不需要显式的free</p></li></ol></li><li><p>constant cache的特点？</p><ol><li>空间大小上限为4KB</li><li>不依赖于threadIdx</li></ol></li></ol><h3 id="如何使用shared-memory？"><a href="#如何使用shared-memory？" class="headerlink" title="如何使用shared memory？"></a>如何使用shared memory？</h3><ol><li><p>有两种方式来使用：静态分配与动态分配</p><ol><li><p>静态分配如下设置：</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="keyword">int</span> shArr[<span class="number">4</span>];</span><br></pre></td></tr></table></figure></li></ol></li><li><p>动态分配可以这样设置：</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="keyword">int</span> shArr[];</span><br><span class="line">kernel&lt;&lt;&lt;grid,block,<span class="keyword">sizeof</span>(<span class="keyword">int</span>)*<span class="number">4</span>&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure></li></ol></li></ol></li><li><p>这种memory 的特点？</p><ol><li><img src="https://lh3.googleusercontent.com/-0wOJ6oyRp14/XD1KCIF0C8I/AAAAAAAANsQ/Q44B4ty6zoce_X5HtgdGZDs48e8oKGE6wCHMYCw/s0/Acrobat_2019-01-15_10-48-40.png" alt=""></li><li>L1cache与shared Memory共用一块存储空间（直接说明速度很快）</li><li>大小有限制</li></ol></li></ol><h3 id="什么是local-memory？在什么情况下变量会存在local-memory中？"><a href="#什么是local-memory？在什么情况下变量会存在local-memory中？" class="headerlink" title="什么是local memory？在什么情况下变量会存在local memory中？"></a>什么是local memory？在什么情况下变量会存在local memory中？</h3><ol><li>local memory是线程中某些变量存储的空间，实质上是global memory中分配给线程的一块内存空间。<ol><li>实质上就是global memory</li></ol></li><li>local memory的特点：<ol><li>线程内私有</li><li>速度很慢（在global memory中）</li></ol></li><li>在这些情况下变量会存在local memory而不存在寄存器中<ol><li>当单个线程中的寄存器不够用的情况下，需要使用local memory存储变量。<ol><li>需要了解单个线程中能够使用的寄存器数量。</li><li><img src="https://lh3.googleusercontent.com/-qquc9S_qCr0/XD1LBKycwvI/AAAAAAAANsc/0QbLFST183gUhJI2FWFgp_d42NWb0dZxACHMYCw/s0/Acrobat_2019-01-15_10-52-52.png" alt=""></li></ol></li><li>如果变量使用到了地址，变量就会存在local memory中。<ol><li><img src="https://lh3.googleusercontent.com/-ZWqsNGN9Kn4/XD1LTH5bTjI/AAAAAAAANso/p4HIh0AdS_ktdH3vwk9rnbb0AAMcY294gCHMYCw/s0/Acrobat_2019-01-15_10-54-05.png" alt=""></li></ol></li><li>使用了递归函数，寄存器显然不够用，当然也会使用local memory</li></ol></li></ol><h3 id="texture-cache"><a href="#texture-cache" class="headerlink" title="texture cache"></a>texture cache</h3><p>TODO:</p><p>对这个没有什么兴趣，就先不看吧。</p><h2 id="GPU中的存储访问模式"><a href="#GPU中的存储访问模式" class="headerlink" title="GPU中的存储访问模式"></a>GPU中的存储访问模式</h2><p>问题：怎样的存储访问模式，效率更高？</p><h3 id="如何确定访问过程中对Global-Memory的访问长度"><a href="#如何确定访问过程中对Global-Memory的访问长度" class="headerlink" title="如何确定访问过程中对Global Memory的访问长度"></a>如何确定访问过程中对Global Memory的访问长度</h3><ol><li><p>基于这样的特性：</p><ol><li>对全局存储总是按32B，64B，128B的大小对齐访问，即使只需要一个字节</li></ol></li><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>找出最小编号活动线程寻址的存储器片段。段的长度由线程访问的字的长度决定：</span><br><span class="line">    * <span class="number">1</span>字节的字<span class="number">32</span>字节</span><br><span class="line">    * <span class="number">2</span>字节的字<span class="number">64</span>字节</span><br><span class="line">    * <span class="number">4</span>，<span class="number">8</span>，<span class="number">16</span>字节的字<span class="number">128</span>字节</span><br><span class="line"><span class="number">2.</span>找出其它地址在同一段内的活动线程</span><br><span class="line"><span class="number">3.</span>减小事务长度，如果可能：</span><br><span class="line">    * 如果事务是<span class="number">128</span>字节且只有下半部分或上半部分被使用，减小事务到<span class="number">64</span>字节；</span><br><span class="line">    * 如果事务是<span class="number">64</span>字节（原始的或者从<span class="number">128</span>字节减小后的）且只有上半部分或下</span><br><span class="line"><span class="number">4.</span>执行事务且标记已访问数据的线程为非活动的。</span><br></pre></td></tr></table></figure></li></ol><h3 id="Global-Memory中，怎样访存效率更高？"><a href="#Global-Memory中，怎样访存效率更高？" class="headerlink" title="Global Memory中，怎样访存效率更高？"></a>Global Memory中，怎样访存效率更高？</h3><ol><li>cuda访存的特性<ol><li>对全局存储总是按32B，64B，128B的大小对齐访问，即使只需要一个字节</li></ol></li><li>根据特性，判断不同访存方式的效率区别(<ol><li>图中例子为：遍历访问数组的32个int元素（32*Bytes=128Bytes）。</li><li>对齐且顺序：<ol><li><img src="https://lh3.googleusercontent.com/-hivZZjGGfi8/XD1PT_H5jRI/AAAAAAAANtA/Jyp5zvfIxyou6Epbh3d0XFTV0Fta6mDCgCHMYCw/s0/Acrobat_2019-01-15_11-11-11.png" alt=""></li></ol></li><li>对齐，但交叉次序访问，注意到对该情况的优化在不同的计算能力下不同。<ol><li>发现1.0的时候，完全不支持交叉次序访问的并行<ol><li>每个int读取一次，32次读取每次读取单位为32B</li></ol></li><li><img src="https://lh3.googleusercontent.com/-8do7xPatAfQ/XD1PgsjTPuI/AAAAAAAANtE/ywf_tP6vUiYlChN0xGN1pzx99kevZeBbQCHMYCw/s0/Acrobat_2019-01-15_11-12-02.png" alt=""></li></ol></li><li>未对齐，但顺序访问<ol><li><img src="https://lh3.googleusercontent.com/-UDvD96XD-Vw/XD1Qa2WOBBI/AAAAAAAANtQ/KiFbNr6zvto_TO4hnaahHwZpzwSHEISxACHMYCw/s0/Acrobat_2019-01-15_11-15-55.png" alt=""></li><li><img src="https://lh3.googleusercontent.com/-wuvaBvFBL04/XD1ST3z6tkI/AAAAAAAANts/LLtskN0255wkfZopm6VnnbL2EvdgPm_iQCHMYCw/s0/Acrobat_2019-01-15_11-23-58.png" alt=""></li></ol></li><li>即使对齐了，但是如果乘上一个系数<ol><li><img src="https://lh3.googleusercontent.com/-yGL6OLkxFsA/XD1QfeEWqFI/AAAAAAAANtU/3xmBtQvVAxMfOlW-EA_UxqEoiWl0aKA2ACHMYCw/s0/Acrobat_2019-01-15_11-16-14.png" alt=""></li></ol></li><li>随机访问<ol><li><img src="https://lh3.googleusercontent.com/-rCGJm6lllIs/XD1Qrdz7GbI/AAAAAAAANtc/EyEVB2TTgko4nkQ0ciJ35v3VjZfYNNE7wCHMYCw/s0/Acrobat_2019-01-15_11-17-01.png" alt=""></li></ol></li></ol></li><li>结论：尽可能对齐且顺序访问</li></ol><h3 id="Constant-Memory中，如何访存效率更高？"><a href="#Constant-Memory中，如何访存效率更高？" class="headerlink" title="Constant Memory中，如何访存效率更高？"></a>Constant Memory中，如何访存效率更高？</h3><ol><li>特点：<ol><li>片外存储器，速度虽然比shared满，但是具有缓存</li><li>只读</li><li>无需考虑冲突问题</li></ol></li><li>访问优化：<ol><li>关键：如果half-warp中的线程访问的不是同一个地址，那么各个线程的访问将会串行化。<img src="https://lh3.googleusercontent.com/-yYJUOX-zNgk/XD1TgLgRnyI/AAAAAAAANt4/jB6R5v6BWuA3RcIbegH_yCq33uPjxdJPACHMYCw/s0/Acrobat_2019-01-15_11-29-05.png" alt=""></li><li>例子：<img src="https://lh3.googleusercontent.com/-cXQbxBAYxWA/XD1TdHGWvJI/AAAAAAAANt0/i-ImhB4Jn98B3IPZObx9llbtihPgvRZdQCHMYCw/s0/Acrobat_2019-01-15_11-28-52.png" alt=""></li></ol></li></ol><h3 id="Shared-Memory中如何访存效率更高？"><a href="#Shared-Memory中如何访存效率更高？" class="headerlink" title="Shared Memory中如何访存效率更高？"></a>Shared Memory中如何访存效率更高？</h3><ol><li><p>特点：</p><ol><li>速度极快（毕竟在L1 cache上）</li><li>如果发生bank冲突，可能会使访存串行化</li></ol></li><li><p>shared memory的硬件结构特点与bank</p><ol><li>参考下图<ol><li>线性编址</li></ol></li><li>（以下图为例的话），地址<code>0008</code>，<code>0048</code>，<code>0088</code>在同一个bank上，无法在一个时钟周期内访问，必须串行访问。例如下图：同一个时间里，half-warp发出的访存请求，如果访问同一个bank上的地址，会导致访存串行化<ol><li><img src="https://lh3.googleusercontent.com/-SA4qmGTqTyM/XD1VSMTUETI/AAAAAAAANuI/FDzh1lkWp04cF8K-eT_yKmE2CKOI3K7xQCHMYCw/s0/Acrobat_2019-01-15_11-36-35.png" alt=""></li></ol></li><li>地址<code>0000</code>，<code>0004</code>，<code>0008</code>，等，在同一个bank上，多个线程可以在同一个时钟周期访问，因此实现了并行访存<ol><li><img src="https://lh3.googleusercontent.com/-Njk-efGNx5M/XD1Vb6wjVtI/AAAAAAAANuM/G6tcHtm7i8cz0KITIjuWuOOH07q2eGaBQCHMYCw/s0/Acrobat_2019-01-15_11-37-17.png" alt=""></li></ol></li></ol></li><li><p>关键优化需要特性</p><ol><li>一个half-warp中的线程，在没有发生bank冲突的情况下，可以在一个时钟周期内访问16个不同的地址。</li><li>一个half-warp中的所有线程如果都访问同一个地址，通过广播机制，可以在同一个时钟周期内完成访问。<img src="https://lh3.googleusercontent.com/-whTB6O25Bz8/XD1Ww8ozsKI/AAAAAAAANuc/I7fX9lzV3ioWMGs93a3-w3-P51tgUgalACHMYCw/s0/Acrobat_2019-01-15_11-42-59.png" alt=""></li></ol></li><li><p>访存例子：</p><ol><li><p><code>v = arr[ threadIdx.x ]</code></p><ol><li>连续的16个元素可以通过一个时钟周期一次访问。</li><li><img src="https://lh3.googleusercontent.com/-d7AP5GuL2ns/XD1YgQ30h5I/AAAAAAAANuo/jGCLvbPigYQHcNE9eJ2h9mMBS9b5K3IJQCHMYCw/s0/Acrobat_2019-01-15_11-50-25.png" alt=""></li></ol></li><li><p><code>v = arr[ threadIdx.x+2 ]</code></p><ol><li>没有发生bank冲突，连续的16个值依然可以通过一个时钟周期一次完成访问。</li><li><img src="https://lh3.googleusercontent.com/-NwTzxZl9Nbc/XD1YsH6m5WI/AAAAAAAANus/fAW7EWnFt2EaKdavF_-lrcCNJ5AFqi1IgCHMYCw/s0/Acrobat_2019-01-15_11-51-13.png" alt=""></li></ol></li><li><code>v = arr[2*threadIdx.x]</code>      <ol><li>连续的16个值，前8个可以一次访问，后8个由于与前8个发生了bank冲突，需要等到下一个时钟周期，需要两个时钟周期</li><li><img src="https://lh3.googleusercontent.com/-XH1ZOdJzB4Y/XD1ZBpizwpI/AAAAAAAANu4/WX7e_gTepj8a7_sWcTEKN69YVmUgWUwjwCHMYCw/s0/Acrobat_2019-01-15_11-52-39.png" alt=""></li></ol></li><li><code>v = arr2[threadIdx.x].x</code><ol><li>注意到arr2数组是由int2类型的结构体（int x, int y）组成的，因此在实际访存中，这个语句有着与上面那个类似的效果。</li><li><img src="https://lh3.googleusercontent.com/-PSr-FTdqRlw/XD1Zqeo80mI/AAAAAAAANvA/HyZcNu2u-mM5N_8ugMuuqYKzpFGKejdhACHMYCw/s0/Acrobat_2019-01-15_11-55-21.png" alt=""></li></ol></li><li><code>v = arr[3*threadIdx.x]</code><ol><li>发现很神奇的是，乘上3，就不会发生冲突了。<ol><li>(3与16互素，x $\in [0,15]$是16的一个剩余系，那么3*x能够遍历16的剩余系)</li></ol></li><li><img src="https://lh3.googleusercontent.com/-UBKhzjGNwco/XD1Z2oL1nZI/AAAAAAAANvE/ZIoh-iVj6ms5LXzrq_jD8Dj1IuzuxXQhwCHMYCw/s0/Acrobat_2019-01-15_11-56-10.png" alt=""></li></ol></li><li><code>v = arr[random]</code><ol><li><img src="https://lh3.googleusercontent.com/-_YfC9QLNbVA/XD1rxZMXNII/AAAAAAAANvY/kp120Lh9CnMiCuAP-wbwn44bSi_QYiVywCHMYCw/s0/Acrobat_2019-01-15_13-12-36.png" alt=""></li></ol></li></ol></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://lh3.googleusercontent.com/-hXWKX0lEAvM/XD1wcULablI/AAAAAAAANvk/auzOo_Af5dAKzdR5htukmRwdnKq15TYyQCHMYCw/s0/Acrobat_2019-01-15_13-32-33.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习5-2-CUDA访存&quot;&gt;&lt;a href=&quot;#HPC复习5-2-CUDA访存&quot; class=&quot;headerlink&quot; title=&quot;HPC复习5.2-CUDA访存&quot;&gt;&lt;/a&gt;HPC复习5.2-CUDA访存&lt;/h1&gt;&lt;p&gt;这里就主要对cuda中的访存模式进行比较详细地说明吧。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GPU中，5种不同存储部件的特性及使用方式&lt;/li&gt;
&lt;li&gt;GPU中，如何使用合并访存加速&lt;/li&gt;
&lt;li&gt;下图是GPU中的存储设备的大图&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/-FME8Iu_B5cI/XD1D5vIEeaI/AAAAAAAANsE/9ObTV12e34ArBzh9o90oEjCsNBJ_X1LZQCHMYCw/s0/Acrobat_2019-01-15_10-22-29.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习5.1-CUDA基础</title>
    <link href="https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A05-1-CUDA%E5%9F%BA%E7%A1%80/"/>
    <id>https://wwyf.github.io/2019/01/15/2019-01-2019-01-15-高性能计算基础-复习5-1-CUDA基础/</id>
    <published>2019-01-15T02:03:26.000Z</published>
    <updated>2019-01-15T02:04:25.938Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习5-1-CUDA基础"><a href="#HPC复习5-1-CUDA基础" class="headerlink" title="HPC复习5.1-CUDA基础"></a>HPC复习5.1-CUDA基础</h1><p>复习了一下cuda，主要以自问自答的方式，整理了一下知识点。</p><ol><li>相关背景：前言</li><li>逻辑上的cuda架构大概是怎样的？</li><li>gpu上实际的硬件情况</li><li>cuda架构与硬件之间的关系</li><li>一个简单实例：矩阵相乘的简单实现</li></ol><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="为什么需要gpu？"><a href="#为什么需要gpu？" class="headerlink" title="为什么需要gpu？"></a>为什么需要gpu？</h3><ol><li>CPU处理能力不断强大，但在进入3D时代后，人们发现庞大的3D图像处理数据计算使得CPU越来越不堪重荷，并且远远超出其计算能力；</li><li>图形计算需求日益增多，作为计算机的显示芯片也飞速发展。图形，图像计算等计算的功能被脱离出来，单独设计为一块芯片——GPU （也就是显卡）。</li></ol><h3 id="gpu与cpu的主要区别在？"><a href="#gpu与cpu的主要区别在？" class="headerlink" title="gpu与cpu的主要区别在？"></a>gpu与cpu的主要区别在？</h3><ol><li>gpu采用了大量的执行单元，并且一个控制单元可以同时控制多个执行单元进行计算，实现类似于SIMD的加速。</li><li><img src="https://lh3.googleusercontent.com/-U6dLqQSsc_U/XD0vGdcz0oI/AAAAAAAANqg/skeMgQgw4NMZnsCU87OTm8bNWqoBnac0ACHMYCw/s0/Acrobat_2019-01-15_08-53-44.png" alt=""></li></ol><h3 id="cuda是什么？"><a href="#cuda是什么？" class="headerlink" title="cuda是什么？"></a>cuda是什么？</h3><ol><li>是一种专门针对GPU的开发工具，可以使用类C语言进行通用计算。</li><li>用于编写host+device异构并行C应用程序</li><li><img src="https://lh3.googleusercontent.com/-FlqXBhfrtAQ/XD0vjzixC7I/AAAAAAAANqo/F_57eojX4nAF3CnPqXjbN-gl2nDc0mk0wCHMYCw/s0/Acrobat_2019-01-15_08-55-43.png" alt=""></li></ol><h2 id="CUDA-架构（逻辑上）"><a href="#CUDA-架构（逻辑上）" class="headerlink" title="CUDA 架构（逻辑上）"></a>CUDA 架构（逻辑上）</h2><h3 id="CUDA中线程的组织方式？"><a href="#CUDA中线程的组织方式？" class="headerlink" title="CUDA中线程的组织方式？"></a>CUDA中线程的组织方式？</h3><ol><li>三级：Grid-Block-Thread<ol><li>Thread ：单个线程，是并行的基本单位</li><li>Block：互相合作的线程组</li><li>Grid：一组Block</li></ol></li><li>需要关注到，一个kernel对应一个Grid</li><li><img src="https://lh3.googleusercontent.com/-4LwbA1Aj2MQ/XD0v2jCDuHI/AAAAAAAANq0/64pcOu7Onqc837_4Q79DnfvvoSzWXocaQCHMYCw/s0/Acrobat_2019-01-15_08-56-58.png" alt=""></li></ol><h3 id="CUDA中的访存模式？"><a href="#CUDA中的访存模式？" class="headerlink" title="CUDA中的访存模式？"></a>CUDA中的访存模式？</h3><p>线程可以访问以下空间</p><ol><li>以线程为单位的<ol><li>线程有内部的<strong>寄存器</strong></li><li>在寄存器不够用的情况下，可以在<strong>Global Memory</strong>中申请一块内存空间作为<strong>Local Memory</strong></li></ol></li><li>以Block为单位的<ol><li>单个block中的线程共享<strong>Shared Memory</strong></li></ol></li><li>以Grid为单位的<ol><li>一个Grid中的所有线程共享<strong>Glocal Memory</strong></li><li>特殊的，一个Grid中的所有线程共享<strong>只读的</strong> <strong>constant memory</strong>(常量存储器),<strong>texture memory</strong>(纹理存储器)</li></ol></li><li>图示如下：<img src="https://lh3.googleusercontent.com/-7Id_bAjJjD4/XD0xXYp4HTI/AAAAAAAANrA/g7kkEfzz_Zgxa6JaHZhGRdDckKAqbXEfACHMYCw/s0/Acrobat_2019-01-15_09-03-24.png" alt=""></li></ol><h2 id="cuda与硬件的关系"><a href="#cuda与硬件的关系" class="headerlink" title="cuda与硬件的关系"></a>cuda与硬件的关系</h2><h3 id="cuda中有哪些硬件？是如何组织的？"><a href="#cuda中有哪些硬件？是如何组织的？" class="headerlink" title="cuda中有哪些硬件？是如何组织的？"></a>cuda中有哪些硬件？是如何组织的？</h3><p>由低到高分别是：</p><ol><li>SP：流处理器</li><li>SM：流多处理器</li><li>TPC：线程处理集群</li><li>SPA：流处理器阵列</li></ol><p><img src="https://lh3.googleusercontent.com/-qmYboKb3JQM/XD0xsCp_K6I/AAAAAAAANrI/Hkgu4X-fusoYYnf2q3B-DWv1BcjVEPLYQCHMYCw/s0/Acrobat_2019-01-15_09-04-49.png" alt=""></p><h3 id="cuda架构与实际硬件的关系？"><a href="#cuda架构与实际硬件的关系？" class="headerlink" title="cuda架构与实际硬件的关系？"></a>cuda架构与实际硬件的关系？</h3><ol><li>Grid：运行在SPA上</li><li>Block的执行方式：<ol><li>一般的cuda应用程序具有多个Block组成的线程组，这些Block会分配到多个SM核上分别执行。</li><li>怎么分配？见下图：<img src="https://lh3.googleusercontent.com/-Plh4mq88QdE/XD0y2ywS9aI/AAAAAAAANrU/w50XC5epme4HVSkv4vKG0C-aRc06ujnmwCHMYCw/s0/Acrobat_2019-01-15_09-09-46.png" alt=""></li><li>注意到这里的分配，会受到以下两个限制的影响：<ol><li>一个SM核上分配的Blcok数量是有限制，G80中的SM核最多8个block</li><li>G80中的SM核最多768个线程。</li></ol></li></ol></li><li>SM核上具有多个Block需要运行后，这些线程更具体的，是如何执行的呢？<ol><li>一个SM核上的多个Block，每个block会分成多个Warp（32个线程）</li><li>这些Warp会在SM核上并发地执行，由于一个Warp有32个线程，而一个SM核上仅有8个SP，因此一个Warp运行4个Clock cycles</li></ol></li></ol><h3 id="Warp的调度具有开销吗？"><a href="#Warp的调度具有开销吗？" class="headerlink" title="Warp的调度具有开销吗？"></a>Warp的调度具有开销吗？</h3><ol><li>（联想）cpu中的硬件多线程之间的调度是几乎没有开销的，原因？是因为CPU中已经有了多个可以用于存储线程context的区域，每次调度切换线程的时候，切换context是在CPU硬件内完成的，不是由操作系统完成的，不需要访存，因此几乎0开销。</li><li>GPU中的warp调度，类似的，也是0开销的。</li></ol><h3 id="怎样的Warp会被调度出来执行？"><a href="#怎样的Warp会被调度出来执行？" class="headerlink" title="怎样的Warp会被调度出来执行？"></a>怎样的Warp会被调度出来执行？</h3><ol><li>很明显的，每一个时刻，都会有很多个Warp在等待被GPU调度执行，那么问题是：满足什么条件的Warp，会被调度出来执行？<ol><li>Warp中没有线程被阻塞的（如访存等）</li><li>合适的Warp挑出来优先执行。</li></ol></li></ol><h3 id="为什么要设计成warp的并发执行？"><a href="#为什么要设计成warp的并发执行？" class="headerlink" title="为什么要设计成warp的并发执行？"></a>为什么要设计成warp的并发执行？</h3><ol><li>结论：Warp的并发执行，能够很好的隐藏访存时间（原理类似于cpu中的硬件多线程）</li><li>简单例子：<ol><li><img src="https://lh3.googleusercontent.com/-M9QwhzmB0pI/XD02cA_ftZI/AAAAAAAANrg/NpkbQH9Eo2I7kq1FcXk2uzkxt10DuWbUgCHMYCw/s0/Acrobat_2019-01-15_09-25-04.png" alt=""></li><li>上图中，一个单位长度为一个warp的执行（实际上其实是4个时钟周期，这里简化成了1个）。</li><li>可以发现，每当一个Warp由于访存阻塞了，GPU会马上从调度其他可用的Warp来执行，从而保证GPU中的计算负载保持在100%，这个Warp的访存时间就被别的Warp的执行隐藏掉了</li></ol></li><li>复杂例子：如何计算完全隐藏访存时间所需要的Warp的数量。<ol><li>假设：<ol><li>运行一次一个Warp中的所有线程需要4个clock cycles</li><li>每n个指令需要一次全局内存访问（200个时钟周期）</li></ol></li><li>解答：<ol><li>假设每个指令都需要访存，访存的这一段时间里，可以使用$200/4=50$个warp的执行来隐藏。<ol><li>隐藏假设：单个Warp一次运行仅使用了4个clock cycles就被阻塞了</li></ol></li><li>由于是每n个指令访存一次，因此实际上，单个warp执行了$4*n$后才会被阻塞</li><li>因此，需要$200/(4*n)+1$个warp。</li></ol></li></ol></li></ol><h3 id="SM核上具有的存储空间及分配情况？"><a href="#SM核上具有的存储空间及分配情况？" class="headerlink" title="SM核上具有的存储空间及分配情况？"></a>SM核上具有的存储空间及分配情况？</h3><ol><li>SM核上拥有16KB的shared memory(有些GPU是48KB)</li><li>基于前面，一个SM核上分配多个Block的前提<ol><li>多个block分享一个SM核上的shared memory。</li></ol></li><li>由于单个block可能会要求shared memory至少多大，而一个SM核上的shared memory是有限制的，所以，block要看情况，不能分配太多，不然shared memory就不够用了。<ol><li>Shared Memory也会限制Block的分配</li></ol></li></ol><h2 id="cuda的软件接口"><a href="#cuda的软件接口" class="headerlink" title="cuda的软件接口"></a>cuda的软件接口</h2><p>写了这么多，终于写到要写一个真正的cuda程序了。</p><h3 id="cuda编程中的函数声明"><a href="#cuda编程中的函数声明" class="headerlink" title="cuda编程中的函数声明"></a>cuda编程中的函数声明</h3><ol><li>在cuda编程中，当然既要写在gpu上运行的函数又要写在cpu上运行的函数了，那么如何区分呢？</li><li><img src="https://lh3.googleusercontent.com/-Frw4biWiTdw/XD063lkpXrI/AAAAAAAANrs/2FySYbhR3EsgK3TcHH6cPOlsy7x_zxzTQCHMYCw/s0/Acrobat_2019-01-15_09-43-58.png" alt=""></li><li>好吧，之前我没有好好想<code>__device__</code>还有<code>__global__</code>的区别，所以这里要特地给自己强调一下。</li></ol><h3 id="cuda中的5个内建设备变量"><a href="#cuda中的5个内建设备变量" class="headerlink" title="cuda中的5个内建设备变量"></a>cuda中的5个内建设备变量</h3><ol><li><code>gridDim</code></li><li><code>blockDim</code></li><li><code>blockIdx</code></li><li><code>threadIdx</code></li><li><code>warpSize</code></li></ol><h3 id="cuda中的各种各样的内存操作"><a href="#cuda中的各种各样的内存操作" class="headerlink" title="cuda中的各种各样的内存操作"></a>cuda中的各种各样的内存操作</h3><p><code>cudaMemcpy(void * dst, void * src, size_t nbytes, enum cudaMemcpyType direction);</code></p><p><code>cudaMalloc</code></p><p><code>cudaFree</code></p><h3 id="cuda中的同步操作"><a href="#cuda中的同步操作" class="headerlink" title="cuda中的同步操作"></a>cuda中的同步操作</h3><p><code>__syncThreads()</code>：同步一个block里面的所有线程，作用相当于一个Barrier</p><h2 id="实例：very-simple的矩阵相乘"><a href="#实例：very-simple的矩阵相乘" class="headerlink" title="实例：very simple的矩阵相乘"></a>实例：very simple的矩阵相乘</h2><p>可以看我的<a href="https://gitee.com/wwyf/class_hpc/blob/master/e6/code/multi.cu" target="_blank" rel="noopener">git仓库</a>（我具体的实现可能没有这么naive）</p><p>具体思路是：</p><p><img src="https://lh3.googleusercontent.com/-3jJ-qYk5SD8/XD09D8JLBTI/AAAAAAAANr4/HjWn0Aj36A4S0xuZEoZ70EqhyJ6waYYGwCHMYCw/s0/Acrobat_2019-01-15_09-53-19.png" alt=""></p><p>对该程序优化的思考</p><ol><li>性能问题：<ol><li>每计算一次，访问两次内存（如从矩阵Md中取一个数，以及从Nd中取一个数，然后只做了一次加法）</li><li>访存限制</li></ol></li><li>矩阵的大小受到一个block大小的限制。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习5-1-CUDA基础&quot;&gt;&lt;a href=&quot;#HPC复习5-1-CUDA基础&quot; class=&quot;headerlink&quot; title=&quot;HPC复习5.1-CUDA基础&quot;&gt;&lt;/a&gt;HPC复习5.1-CUDA基础&lt;/h1&gt;&lt;p&gt;复习了一下cuda，主要以自问自答的方式，整理了一下知识点。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;相关背景：前言&lt;/li&gt;
&lt;li&gt;逻辑上的cuda架构大概是怎样的？&lt;/li&gt;
&lt;li&gt;gpu上实际的硬件情况&lt;/li&gt;
&lt;li&gt;cuda架构与硬件之间的关系&lt;/li&gt;
&lt;li&gt;一个简单实例：矩阵相乘的简单实现&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习3-pthread</title>
    <link href="https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A03-pthread/"/>
    <id>https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-高性能计算基础-复习3-pthread/</id>
    <published>2019-01-14T13:38:29.000Z</published>
    <updated>2019-01-16T01:15:19.718Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习3-pthread"><a href="#HPC复习3-pthread" class="headerlink" title="HPC复习3-pthread"></a>HPC复习3-pthread</h1><p><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html" target="_blank" rel="noopener">toturial</a></p><p>这里是高性能计算课程-pthread部分的复习笔记，大概会有以下内容：</p><ol><li>pthread中的hello,world</li><li>pthread中的临界区</li><li>忙等待</li><li>互斥量</li><li>生产者-消费者同步与信号量</li><li>实现路障</li><li>读写锁与链表</li><li>pthread中的缓存一致性</li></ol><a id="more"></a><h2 id="进程、线程和pthread"><a href="#进程、线程和pthread" class="headerlink" title="进程、线程和pthread"></a>进程、线程和pthread</h2><ol><li>pthread:一种共享内存编程模型</li></ol><h2 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello,world"></a>Hello,world</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pthread.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">print_message_function</span><span class="params">( <span class="keyword">void</span> *ptr )</span></span>;</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line">&#123;</span><br><span class="line">     <span class="keyword">pthread_t</span> thread1, thread2;</span><br><span class="line">     <span class="keyword">char</span> *message1 = <span class="string">"Thread 1"</span>;</span><br><span class="line">     <span class="keyword">char</span> *message2 = <span class="string">"Thread 2"</span>;</span><br><span class="line">     <span class="keyword">int</span>  iret1, iret2;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Create independent threads each of which will execute function */</span></span><br><span class="line"></span><br><span class="line">     iret1 = pthread_create( &amp;thread1, <span class="literal">NULL</span>, print_message_function, (<span class="keyword">void</span>*) message1);</span><br><span class="line">     iret2 = pthread_create( &amp;thread2, <span class="literal">NULL</span>, print_message_function, (<span class="keyword">void</span>*) message2);</span><br><span class="line"></span><br><span class="line">     <span class="comment">/* Wait till threads are complete before main continues. Unless we  */</span></span><br><span class="line">     <span class="comment">/* wait we run the risk of executing an exit which will terminate   */</span></span><br><span class="line">     <span class="comment">/* the process and all threads before the threads have completed.   */</span></span><br><span class="line"></span><br><span class="line">     pthread_join( thread1, <span class="literal">NULL</span>);</span><br><span class="line">     pthread_join( thread2, <span class="literal">NULL</span>); </span><br><span class="line"></span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">"Thread 1 returns: %d\n"</span>,iret1);</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">"Thread 2 returns: %d\n"</span>,iret2);</span><br><span class="line">     <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">print_message_function</span><span class="params">( <span class="keyword">void</span> *ptr )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">char</span> *message;</span><br><span class="line">     message = (<span class="keyword">char</span> *) ptr;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">"%s \n"</span>, message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="pthread中的临界区"><a href="#pthread中的临界区" class="headerlink" title="pthread中的临界区"></a>pthread中的临界区</h2><ol><li>课本上使用多个线程并行计算$\pi$会出问题<ol><li>多个线程尝试更新同一个共享变量时，会出问题。</li></ol></li><li>多个线程尝试更新一个共享资源，结果可能是无法预测的，这些访问可能会导致某种错误，我们称之为<strong>竞争条件</strong></li><li><strong>临界区</strong>：更新共享资源的代码段</li></ol><h2 id="忙等待"><a href="#忙等待" class="headerlink" title="忙等待"></a>忙等待</h2><ol><li>可以使用忙等待实现“严格按照线程号，单个线程进入临界区”。</li><li>注意，可能会由于发生了编译优化导致该忙等待失效（该忙等待语句可能会调度到其他指令前后）<ol><li>可以通过<code>volitile</code>关键字来解决</li></ol></li><li>缺点：<ol><li>忙等待：浪费CPU周期</li></ol></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initialize</span></span><br><span class="line">flag = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// .... some code</span></span><br><span class="line"><span class="keyword">while</span>(flag != my_rank);</span><br><span class="line"><span class="comment">// critical area</span></span><br><span class="line">flag = (flag + <span class="number">1</span>)% thread_count;<span class="comment">// 保证在所有进程都已到达后，flag恢复成0</span></span><br></pre></td></tr></table></figure><h2 id="互斥量"><a href="#互斥量" class="headerlink" title="互斥量"></a>互斥量</h2><ol><li>改善忙等待的缺点：互斥量，互斥锁</li><li>设计函数接口<ol><li><code>pthread_mutex_init( pthread_mutex_t *, const pthread_mutexattr_t *)</code></li><li><code>int pthread_mutex_destroy(pthread_mutex_t* )</code></li><li><code>int pthread_mutex_lock(pthread_mutex_t *)</code></li><li><code>int pthread_mutex_unlock(pthread_mutex_t *)</code></li></ol></li></ol><h2 id="生产者-消费者同步和信号量"><a href="#生产者-消费者同步和信号量" class="headerlink" title="生产者-消费者同步和信号量"></a>生产者-消费者同步和信号量</h2><p>TODO:</p><h2 id="路障和条件变量"><a href="#路障和条件变量" class="headerlink" title="路障和条件变量"></a>路障和条件变量</h2><ol><li><p>问题：如何保证所有线程在程序中处于同一位置来同步线程（即实现路障）</p></li><li><p>忙等待和互斥量实现路障</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> counter;</span><br><span class="line"><span class="keyword">int</span> thread_count;</span><br><span class="line"><span class="keyword">pthread_mutex_t</span> barier_mutex;</span><br><span class="line"><span class="comment">//....</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> * <span class="title">Threa_word</span><span class="params">(...)</span></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">/* barrier */</span></span><br><span class="line">    pthread_mutex_lock(&amp;barier_mutex);</span><br><span class="line">    counter++；</span><br><span class="line">    pthread_mutex_unlock(&amp;barrier_mutex);</span><br><span class="line">    <span class="keyword">while</span>(counter &lt; thread_count);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>信号量实现路障</p><ol><li><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> counter;</span><br><span class="line"><span class="keyword">sem_t</span> count_sem;</span><br><span class="line"><span class="keyword">sem_t</span> barrier_sem;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">Thread_work</span><span class="params">(...)</span></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">/* barrier */</span></span><br><span class="line">    <span class="comment">// 先获得计数器的信号量</span></span><br><span class="line">    sem_wait(&amp;count_sem);</span><br><span class="line">    <span class="keyword">if</span> (counter == thread_count<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="comment">// 最后一个到达路障的线程负责初始化counter以及释放其他线程</span></span><br><span class="line">        counter = <span class="number">0</span>;</span><br><span class="line">        sem_post(&amp;count_sem);</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; thread_count<span class="number">-1</span>; j++)</span><br><span class="line">            sem_post(&amp;barrier_sem);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        counter++;</span><br><span class="line">        sem_postq(&amp;count_sem);</span><br><span class="line">        sem_wait(&amp;barrier_sem);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>该路障的重用可能会导致竞争条件</p></li></ol></li><li><p>使用条件变量实现路障</p><ol><li><p>条件变量是：允许线程在某个特定条件或事前发生前都处于挂起状态。当事件发生时，另一个线程可以通过信号来唤醒挂起的线程。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> * <span class="title">Thread_work</span><span class="params">(...)</span></span>&#123;</span><br><span class="line">    ....</span><br><span class="line">    <span class="comment">/* barrier */</span></span><br><span class="line">    pthread_mutex_lock(&amp;mutex);</span><br><span class="line">    counter++;</span><br><span class="line">    <span class="keyword">if</span> (counter == thread_count)&#123;</span><br><span class="line">        counter = <span class="number">0</span>;</span><br><span class="line">        pthread_cond_broadcast(&amp;cond_var);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">while</span>(pthread_cond_wait(&amp;cond_var, &amp;mutex) != <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    pthread_mutex_unlock(&amp;mutex);    </span><br><span class="line">    ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h2><p>使用读写锁，实现多线程共享的链表怎么实现？不难。</p><p><img src="https://lh3.googleusercontent.com/-jm61UeXUUc4/XD6AbNTejpI/AAAAAAAAN2k/AHGevmv9O5kY5oRH6DnqEail-W6a8NiIgCHMYCw/s0/Acrobat_2019-01-16_08-53-00.png" alt=""></p><p>一个结论：在Insert，Delete操作十分少的时候，使用读写锁的性能更好。</p><p><img src="https://lh3.googleusercontent.com/-BgLlpcl5A-k/XD6ATQaRnAI/AAAAAAAAN2g/DlvaYc1Cp5kLj6O4rNPIOQtAL_rVe-nigCHMYCw/s0/Acrobat_2019-01-16_08-52-27.png" alt=""></p><h2 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h2><p>书本拿了矩阵-向量乘法的例子，说明了缓存对程序性能的影响</p><ol><li>对于8*8000000的矩阵，伪共享带来了很大的影响<ol><li>关键：$y[0]-y[7]$在同一个缓存行中</li></ol></li></ol><p><img src="https://lh3.googleusercontent.com/-LJMIfdNhooo/XDyQgxv4XJI/AAAAAAAANqI/8lBb1o5SPBUV1Gn6uK0sRuGp0bH87hCJQCHMYCw/s0/Typora_2019-01-14_21-37-07.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习3-pthread&quot;&gt;&lt;a href=&quot;#HPC复习3-pthread&quot; class=&quot;headerlink&quot; title=&quot;HPC复习3-pthread&quot;&gt;&lt;/a&gt;HPC复习3-pthread&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;toturial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里是高性能计算课程-pthread部分的复习笔记，大概会有以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pthread中的hello,world&lt;/li&gt;
&lt;li&gt;pthread中的临界区&lt;/li&gt;
&lt;li&gt;忙等待&lt;/li&gt;
&lt;li&gt;互斥量&lt;/li&gt;
&lt;li&gt;生产者-消费者同步与信号量&lt;/li&gt;
&lt;li&gt;实现路障&lt;/li&gt;
&lt;li&gt;读写锁与链表&lt;/li&gt;
&lt;li&gt;pthread中的缓存一致性&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习2-MPI</title>
    <link href="https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A02-MPI/"/>
    <id>https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-高性能计算基础-复习2-MPI/</id>
    <published>2019-01-14T12:26:23.000Z</published>
    <updated>2019-01-14T13:39:25.530Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习2-MPI"><a href="#HPC复习2-MPI" class="headerlink" title="HPC复习2-MPI"></a>HPC复习2-MPI</h1><p><a href="https://www.open-mpi.org/doc/current/" target="_blank" rel="noopener">OpenMPI 官方文档</a></p><p><a href="http://www.mpich.org/static/docs/latest/" target="_blank" rel="noopener">mpich官方文档</a></p><p>这里是高性能计算课程-MPI部分的复习笔记，大概会有以下内容：</p><ol><li>MPI中的Hello world</li><li>常见的MPI函数</li><li>使用MPI实现梯形积分法</li><li>中级：MPI中的集合通信</li><li>中级：MPI中的派生数据类型</li><li>中级：MPI中的计时方法</li><li>算法：奇偶并行排序算法</li><li>算法：并行正则采样排序</li></ol><a id="more"></a><h2 id="MPI中的hello-world"><a href="#MPI中的hello-world" class="headerlink" title="MPI中的hello world"></a>MPI中的hello world</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAX_STRING = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> greeting[MAX_STRING];</span><br><span class="line">    <span class="keyword">int</span> comm_sz;</span><br><span class="line">    <span class="keyword">int</span> my_rank;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize the MPI environment</span></span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">// Get the number of processes</span></span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);</span><br><span class="line">    <span class="comment">// Get the rank of the process</span></span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (my_rank != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">sprintf</span>(greeting, <span class="string">"Greetings from process %d of %d!"</span>, my_rank, comm_sz);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Hello world from ank %d\n"</span>,my_rank);</span><br><span class="line">        MPI_Send(</span><br><span class="line">            greeting,</span><br><span class="line">            <span class="comment">// strlen(greeting)+1,</span></span><br><span class="line">            <span class="comment">// strlen(greeting),</span></span><br><span class="line">            MAX_STRING,</span><br><span class="line">            MPI_CHAR,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            MPI_COMM_WORLD</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Greetings from process %d of %d!\n"</span>, my_rank, comm_sz);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> q = <span class="number">1</span>; q &lt; comm_sz; q++)&#123;</span><br><span class="line">            MPI_Recv(</span><br><span class="line">                greeting,</span><br><span class="line">                MAX_STRING,</span><br><span class="line">                MPI_CHAR,</span><br><span class="line">                q,</span><br><span class="line">                <span class="number">0</span>,</span><br><span class="line">                MPI_COMM_WORLD,</span><br><span class="line">                MPI_STATUS_IGNORE</span><br><span class="line">            );</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s in %d\n"</span>, greeting, my_rank);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Finalize the MPI environment.</span></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="常见的MPI函数"><a href="#常见的MPI函数" class="headerlink" title="常见的MPI函数"></a>常见的MPI函数</h2><p>这6个MPI函数，可以完成一切任务</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Init</span><span class="params">(<span class="keyword">int</span> *argc, <span class="keyword">char</span> **argv[])</span></span>;</span><br><span class="line"><span class="comment">// 进入MPI环境并完成所有的初始化工作</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Finalize</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br><span class="line"><span class="comment">// 从MPI环境中退出</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Comm_rank</span><span class="params">(MPI_Comm comm, <span class="keyword">int</span> *rank)</span></span>;</span><br><span class="line"><span class="comment">// 获得当前进程在指定通信域中的编号</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Comm_size</span><span class="params">(MPI_Comm comm, <span class="keyword">int</span> *size)</span></span>;</span><br><span class="line"><span class="comment">// 获得指定通信域中的进程数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Send</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> dest, <span class="keyword">int</span> tag, MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 发送消息到目标进程</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Recv</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> source, <span class="keyword">int</span> tag, MPI_Comm comm,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Status * status_p</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"><span class="comment">// 从指定进程接受一个消息</span></span></span><br></pre></td></tr></table></figure><h2 id="使用MPI实现梯形积分法"><a href="#使用MPI实现梯形积分法" class="headerlink" title="使用MPI实现梯形积分法"></a>使用MPI实现梯形积分法</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 对这一个函数进行积分</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x*x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">Trap</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> left_endpt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> right_endpt,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> trap_count,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> base_len</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"><span class="comment">/**</span></span></span><br><span class="line"><span class="function"><span class="comment"> * @brief 梯形积分法的串行实现</span></span></span><br><span class="line"><span class="function"><span class="comment"> * </span></span></span><br><span class="line"><span class="function"><span class="comment"> * @param base_len 就是left_endpt与right_endpt之间分成trap_count份后，每一份的长度</span></span></span><br><span class="line"><span class="function"><span class="comment"> * </span></span></span><br><span class="line"><span class="function"><span class="comment"> */</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> estimate, x;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    estimate = (f(left_endpt) + f(right_endpt))/<span class="number">2.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= trap_count<span class="number">-1</span>; i++)&#123;</span><br><span class="line">        x = left_endpt + i*base_len;</span><br><span class="line">        estimate += f(x);</span><br><span class="line">    &#125;</span><br><span class="line">    estimate = estimate*base_len;</span><br><span class="line">    <span class="keyword">return</span> estimate;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Get_input</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> my_rank,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> comm_sz,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> * a_p,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> * b_p,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>* n_p</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dest;</span><br><span class="line">    <span class="keyword">if</span> (my_rank == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Enter a,b,and n\n"</span>);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%lf %lf %d"</span>, a_p, b_p, n_p);</span><br><span class="line">        <span class="keyword">for</span> (dest = <span class="number">1</span>; dest &lt; comm_sz; dest++)&#123;</span><br><span class="line">            MPI_Send(a_p, <span class="number">1</span>, MPI_DOUBLE, dest, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">            MPI_Send(b_p, <span class="number">1</span>, MPI_DOUBLE, dest, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">            MPI_Send(n_p, <span class="number">1</span>, MPI_INT, dest, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        MPI_Recv(a_p, <span class="number">1</span>, MPI_DOUBLE, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD,MPI_STATUS_IGNORE);</span><br><span class="line">        MPI_Recv(b_p, <span class="number">1</span>, MPI_DOUBLE, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD,MPI_STATUS_IGNORE);</span><br><span class="line">        MPI_Recv(n_p, <span class="number">1</span>, MPI_INT, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD,MPI_STATUS_IGNORE);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"in trap %lf, %lf, %d\n"</span>, *a_p, *b_p, *n_p);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">// DEBUG</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> my_rank, comm_sz, n=<span class="number">1024</span>, local_n;</span><br><span class="line">    <span class="keyword">double</span> a = <span class="number">0.0</span>, b = <span class="number">3.0</span>, h, local_a, local_b;</span><br><span class="line">    <span class="keyword">double</span> local_int, total_int;</span><br><span class="line">    <span class="keyword">int</span> source;</span><br><span class="line"></span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);</span><br><span class="line"></span><br><span class="line">    Get_input(my_rank, comm_sz, &amp;a, &amp;b, &amp;n);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"in main %lf, %lf, %d\n"</span>, a, b, n);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">// DEBUG</span></span></span><br><span class="line">    <span class="comment">// 将积分区间分成n份</span></span><br><span class="line">    h = (b-a)/n;</span><br><span class="line">    <span class="comment">// 将n分区间，分到comm_sz个进程里，每个进程分到local_n个区间</span></span><br><span class="line">    local_n = n/comm_sz;</span><br><span class="line"></span><br><span class="line">    local_a = a+my_rank*local_n*h;</span><br><span class="line">    local_b = local_a+local_n*h;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"local_a : %lf, local_b : %lf, local_n : %d"</span>, local_a, local_b, local_n);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">// DEBUG</span></span></span><br><span class="line">    local_int = Trap(local_a, local_b, local_n, h);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (my_rank != <span class="number">0</span>)&#123;</span><br><span class="line">        MPI_Send(&amp;local_int, <span class="number">1</span>, MPI_DOUBLE, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        total_int = local_int;</span><br><span class="line">        <span class="keyword">for</span> (source = <span class="number">1</span>; source &lt; comm_sz; source++)&#123;</span><br><span class="line">            MPI_Recv(&amp;local_int, <span class="number">1</span>, MPI_DOUBLE, source, <span class="number">0</span>, MPI_COMM_WORLD, MPI_STATUS_IGNORE);</span><br><span class="line">            total_int += local_int;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(my_rank == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"With n = %d trapezoids, out estimate\n"</span>, n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"of the integral from %f to %f = %.15e\n"</span>,a,b,total_int);</span><br><span class="line">    &#125;</span><br><span class="line">    MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="中级：MPI中的阻塞与非阻塞通信"><a href="#中级：MPI中的阻塞与非阻塞通信" class="headerlink" title="中级：MPI中的阻塞与非阻塞通信"></a>中级：MPI中的阻塞与非阻塞通信</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MPI_Send</span><br><span class="line"><span class="comment">// normal send</span></span><br><span class="line">MPI_Isend</span><br><span class="line"><span class="comment">// begin a nonblocking send</span></span><br><span class="line">MPI_Ssend</span><br><span class="line"><span class="comment">// Blocking synchronous send</span></span><br><span class="line"></span><br><span class="line">MPI_Bsend</span><br><span class="line"><span class="comment">// send message wich user-provided buffering</span></span><br><span class="line">MPI_Issend</span><br><span class="line"><span class="comment">// Starts a nonblocking synchronous send</span></span><br><span class="line">MPI_Ibsend</span><br><span class="line"><span class="comment">// Starts a nonblocking buffered send</span></span><br><span class="line">MPI_Rsend</span><br><span class="line"><span class="comment">// Blocking ready send</span></span><br><span class="line">MPI_Irsend</span><br><span class="line"><span class="comment">// Starts a nonblocking ready send</span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MPI_Irecv</span><br><span class="line"><span class="comment">// Begins a nonblocking receive</span></span><br></pre></td></tr></table></figure><h2 id="中级：MPI中的集合通信"><a href="#中级：MPI中的集合通信" class="headerlink" title="中级：MPI中的集合通信"></a>中级：MPI中的集合通信</h2><p>涉及通信子中所有进程的通信函数成为集合通信（与点对点通信区分开）</p><p>MPI中的集合通信，在树中介绍的主要有以下几个函数：</p><table><thead><tr><th>函数名</th><th>作用</th></tr></thead><tbody><tr><td>MPI_Reduce</td><td>归约（可以用来求和等等,支持交换律和结合律的运算）</td></tr><tr><td>MPI_Allreduce</td><td>所有进程都可以得到全局求和的结果</td></tr><tr><td>MPI_Bcast</td><td>广播，顾名思义</td></tr><tr><td>MPI_Scatter</td><td>0号进程读入整个向量，但只将分量发送给需要分量的其他进程</td></tr><tr><td>MPI_Gather</td><td>将其他进程的分量都收集到0号进程</td></tr><tr><td>MPI_Allgather</td><td>将每个进程desend_buf_p内容串联起来，存储到每个进程的recv_buf_p参数中</td></tr></tbody></table><p>下面一个一个函数分别说明其参数情况。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Reduce</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Op op,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> root,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Allreduce</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Op op,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Bcast</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *buffer, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> count, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype datatype, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> root, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Scatter</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> sendcount, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> recvcount, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype recvtype, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> root, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Gather</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> sendcount, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> recvcount, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype recvtype, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> root, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Allgather</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> sendcount,  <span class="comment">// local_n</span></span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> recvcount, <span class="comment">// local_n</span></span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype recvtype, </span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h2 id="中级：MPI中的派生数据类型"><a href="#中级：MPI中的派生数据类型" class="headerlink" title="中级：MPI中的派生数据类型"></a>中级：MPI中的派生数据类型</h2><blockquote><p>在MPI中，通过同时存储数据项的类型以及他们在内存中的相对位置，派生数据类型可以表示内存中数据项的任意集合。</p></blockquote><p>在书本中，派生数据类型用在了，减少通信量上。</p><p>一般创建一个新的派生数据类型，需要进行以下的步骤：</p><ol><li>调用<code>MPI_Type_create_struct</code>函数，创建派生数据类型<ol><li>可使用<code>MPI_Get_address</code>辅助得到相对地址</li></ol></li><li>调用<code>MPI_Type_commit</code>函数，允许MPI实现为了在通信函数内使用这一数据类型，优化数据类型的内部表示</li><li>使用结束后，调用<code>MPI_Type_free</code>函数释放额外的存储空间</li></ol><h3 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Type_create_struct</span><span class="params">(<span class="keyword">int</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">const</span> <span class="keyword">int</span> array_of_blocklengths[],</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">const</span> MPI_Aint array_of_displacements[],</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">const</span> MPI_Datatype array_of_types[],</span></span></span><br><span class="line"><span class="function"><span class="params">                           MPI_Datatype * newtype</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Type_commit</span><span class="params">(MPI_Datatype * datatype_p)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Type_free</span><span class="params">(MPI_Datatype * datatype)</span></span>;</span><br></pre></td></tr></table></figure><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p><img src="https://lh3.googleusercontent.com/-ZQmKH0alUZ4/XDx4AZvqyiI/AAAAAAAANps/UnqSTxTMGJ4OO2Nq86kZFBMXJCN_YKBEQCHMYCw/s0/Acrobat_2019-01-14_19-52-35.png" alt=""></p><h2 id="中级：MPI中的计时方法"><a href="#中级：MPI中的计时方法" class="headerlink" title="中级：MPI中的计时方法"></a>中级：MPI中的计时方法</h2><ol><li><code>MPI_Wtime</code></li><li><code>MPI_Barrier</code></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MPI_Barrier(comm);</span><br><span class="line">local_start = MPI_Wtime();</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">    </span><br><span class="line">local_finish = MPI_Wtime();</span><br><span class="line">local_elapsed = local_finish - local_start;</span><br></pre></td></tr></table></figure><h2 id="算法：并行奇偶交换排序"><a href="#算法：并行奇偶交换排序" class="headerlink" title="算法：并行奇偶交换排序"></a>算法：并行奇偶交换排序</h2><h3 id="奇偶交换排序"><a href="#奇偶交换排序" class="headerlink" title="奇偶交换排序"></a>奇偶交换排序</h3><p>关键思想：去耦的比较-交换</p><ol><li>偶数阶段：以下数对进行比较-交换<ol><li>$(a[0], a[1]), (a[2],a[3]), (a[4],a[5]),…$</li></ol></li><li>奇数阶段：以下数对进行比较-交换<ol><li>$(a[1], a[2]), (a[3],a[4]), (a[5],a[6]),…$</li></ol></li><li>定理：n个值的列表，经过n个阶段后，该列表一定能够排好序。</li></ol><h3 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h3><p>步骤：</p><ol><li>将数据分到不同的进程之后，先本地进行一个qsort排个序</li><li>偶数阶段：进程0,1，进程2,3进行比较-交换数据</li><li>奇数阶段：进程1,2， 进程3,4进行比较-交换数据</li></ol><p><img src="https://lh3.googleusercontent.com/-eZA5pXZnybQ/XDx7VnZQdDI/AAAAAAAANp4/DVSafASPAYgRV2LWJXEz1F-a4z06JGiHQCHMYCw/s0/Acrobat_2019-01-14_20-06-48.png" alt=""></p><h2 id="算法：并行正则采样排序"><a href="#算法：并行正则采样排序" class="headerlink" title="算法：并行正则采样排序"></a>算法：并行正则采样排序</h2><ol><li>数据初始化阶段：每个进程根据进程号与数据量，计算得到本进程所读取的数据范围，并从文件中直接读取。由于读取数据的步骤不需要进行通信分发，提高了程序运行的效率。</li><li>每一个进程对其本地的无序数据,长度为$local_n$的$local_buffer$数组进行串行快速排序，从而在每个处理器上都得到一个有序的序列$local_buffer$。</li><li>在每一个处理器上选取代表元素：每一个处理器从局部有序序列中选取第$w$，第$2<em>w$，第$3</em>w$,第$(comm_sz-1)w$共$p-1$个代表元素，其中$w=comm_sz/(p*p)$。</li><li>进程0收集每一个进程中得到的代表元素，从而具有了$(p-1)<em>(p-1)$个代表元素，然后进程0对所有代表元素进行排序，选取第$comm_sz-1$，第$2</em>(comm_sz-1)$，第$3<em>(comm_sz-1)\ \cdots (comm_sz-1)</em>(comm_sz-1)$个元素，这$comm_sz-1$个元素作为主元。</li><li>进程0将上一步中得到的$comm_sz-1$个主元$pivot_values$，分发到其余所有处理器上。</li><li>局部有序序列划分：每一个处理器根据这$comm_sz-1$个主元，将本地的$local_buffer$划分成$comm_sz$段。</li><li>有序序列的分发：在上一个步骤中的$comm_sz$段序列中，每一个处理器将本地的第$i$段发送给第$i$个处理器，最终处理器$i$拥有所有处理器的第$i$段。</li><li>最终排序：每个处理器对上一步中得到的$comm_sz$段有序序列进行排序，即为最终结果。</li></ol><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><h3 id="向量求和"><a href="#向量求和" class="headerlink" title="向量求和"></a>向量求和</h3><ol><li>块划分:简单的将连续N个分量所构成的块，分配到每个进程中。</li><li>循环划分:采用轮转的方式去分配向量分量</li><li>块-循环划分:用一个循环来分发向量分量所构成的块，而不是分发单个向量分量。</li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li>在什么场景下必须使用消息标签？<ol><li><img src="https://lh3.googleusercontent.com/-ac7MTJh3T3s/XDxVtBXjiPI/AAAAAAAANpI/VWRc8sgUPlIUJBJHvgJjXZCqcIUp9PzJgCHMYCw/s0/Acrobat_2019-01-14_17-26-13.png" alt=""></li><li>这段代码打算传送A的前32个字节进入X,传送B的前16个字节进入Y.但是,如果消息B尽管后发送但先到达进程Q,就会被第一个recv()接收在X中，使用标签就可以避免这种情况</li><li><img src="https://lh3.googleusercontent.com/-9IwMxoVBf2k/XDxV13-N1SI/AAAAAAAANpM/rZe2IYt1JLIzwqtUtfsiS1QiGPCRFt_qgCHMYCw/s0/Acrobat_2019-01-14_17-26-49.png" alt=""></li></ol></li><li>MPI_Send与MPI_Recv的问题<ol><li>Send的精确行为是由MPI实现决定的，MPI_Send可能有不同大小的缓冲区，在发送消息的时候，是使用缓冲区，还是直接阻塞等待发送完成，由“消息截止大小”决定。</li><li>启示：了解实际执行情况，不要做假设。</li></ol></li><li>关于MPI_Reduce调用顺序<ol><li><img src="https://lh3.googleusercontent.com/-yu7mVk_1EZE/XDxuhZnZycI/AAAAAAAANpg/6efvirvvmQc1TStPAXBgC2ryDgv-cMwHgCHMYCw/s0/Acrobat_2019-01-14_19-12-06.png" alt=""></li><li>内存单元的名字与MPI_Reduce的调用匹配无关，函数调用的顺序决定了匹配方式。</li><li>！！不可预测，可能b中存储的值将是$1+2+1=4$，而d中为$2+1+2=5$</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习2-MPI&quot;&gt;&lt;a href=&quot;#HPC复习2-MPI&quot; class=&quot;headerlink&quot; title=&quot;HPC复习2-MPI&quot;&gt;&lt;/a&gt;HPC复习2-MPI&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.open-mpi.org/doc/current/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenMPI 官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.mpich.org/static/docs/latest/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;mpich官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里是高性能计算课程-MPI部分的复习笔记，大概会有以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MPI中的Hello world&lt;/li&gt;
&lt;li&gt;常见的MPI函数&lt;/li&gt;
&lt;li&gt;使用MPI实现梯形积分法&lt;/li&gt;
&lt;li&gt;中级：MPI中的集合通信&lt;/li&gt;
&lt;li&gt;中级：MPI中的派生数据类型&lt;/li&gt;
&lt;li&gt;中级：MPI中的计时方法&lt;/li&gt;
&lt;li&gt;算法：奇偶并行排序算法&lt;/li&gt;
&lt;li&gt;算法：并行正则采样排序&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算基础-复习1-并行硬件与并行软件</title>
    <link href="https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80-%E5%A4%8D%E4%B9%A01-%E5%B9%B6%E8%A1%8C%E7%A1%AC%E4%BB%B6%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%BD%AF%E4%BB%B6/"/>
    <id>https://wwyf.github.io/2019/01/14/2019-01-2019-01-14-高性能计算基础-复习1-并行硬件与并行软件/</id>
    <published>2019-01-14T08:33:45.000Z</published>
    <updated>2019-01-15T05:38:38.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="HPC复习1-并行硬件与并行软件"><a href="#HPC复习1-并行硬件与并行软件" class="headerlink" title="HPC复习1-并行硬件与并行软件"></a>HPC复习1-并行硬件与并行软件</h1><p>主要分为四部分：</p><ol><li>背景介绍</li><li>超算硬件</li><li>超算软件</li><li>编写并行程序</li></ol><a id="more"></a><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><ol><li>冯·诺依曼结构</li><li>进程、多任务和线程</li><li>冯·诺依曼结构的发展</li></ol><h3 id="冯·诺依曼结构"><a href="#冯·诺依曼结构" class="headerlink" title="冯·诺依曼结构"></a>冯·诺依曼结构</h3><ol><li>包括主存，中央处理单元（控制单元，算术逻辑单元ALU），以及主存和CPU之间的互连结构</li><li>冯若依曼瓶颈：主存和CPU之间的分离</li></ol><p><img src="https://lh3.googleusercontent.com/-1a3WDll47y4/XDw4ZfJw_RI/AAAAAAAANnw/o5l-qRgC4zwyRuVTEYDXURYwzhuJsKPHQCHMYCw/s0/Acrobat_2019-01-14_15-21-10.png" alt=""></p><h3 id="进程、多任务和线程"><a href="#进程、多任务和线程" class="headerlink" title="进程、多任务和线程"></a>进程、多任务和线程</h3><ol><li><strong>进程</strong>：是运行着的程序的一个实例</li><li><strong>多任务</strong>：对同时运行多个程序的支持，可真并行，也可时间片轮转</li></ol><h3 id="冯诺依曼结构的发展"><a href="#冯诺依曼结构的发展" class="headerlink" title="冯诺依曼结构的发展"></a>冯诺依曼结构的发展</h3><ol><li>高速缓存：<ol><li>是一片读写极快但是空间很小的存储区域</li><li>根据程序执行与数据访问行为的局部性，存储部分数据</li><li>目的：让数据存取的速度适应CPU的处理速度（简而言之就是加快存取速度）</li><li><img src="https://lh3.googleusercontent.com/-ng0exR_kQ68/XDw66GtmfJI/AAAAAAAANn8/FXjZR74AmNI-Jm1AZjYJ4MMw34-zdtOPwCHMYCw/s0/Acrobat_2019-01-14_15-31-52.png" alt=""></li></ol></li><li>虚拟内存<ol><li>是一种内存管理技术</li><li>解决：所需内存超过物理内存下程序无法执行的问题，以及其他直接使用物理内存可能带来的问题</li></ol></li><li>指令集并行：单处理器上的细粒度并行<ol><li>流水线技术</li><li>多发射技术</li></ol></li><li>硬件多线程<ol><li>在处理器中多开辟几仹线程状态，当线程发生切换时，处理器切换到对应的线程状态执行，在瞬间即可完成，这种方式叫做硬件多线程</li><li>多种粒度的硬件多线程，可以了解一下<ol><li>粗粒度：遇到长时间中断，切换线程</li><li>细粒度：逐个CPU周期轮流切换线程</li><li>同时多线程：多个线程的指令能够被同时发射</li><li><img src="https://lh3.googleusercontent.com/-N8o9Xm48f08/XDw73TB4n_I/AAAAAAAANoE/jIAbyujdxncxl4hMJo7yPu8y_Ptch8fzQCHMYCw/s0/Acrobat_2019-01-14_15-35-58.png" alt=""></li></ol></li></ol></li></ol><h2 id="超算硬件"><a href="#超算硬件" class="headerlink" title="超算硬件"></a>超算硬件</h2><p>主要有以下内容：</p><ol><li>两类并行系统：SIMD，MIMD</li><li>互联网络</li><li>缓存一致性</li></ol><h3 id="两类并行系统"><a href="#两类并行系统" class="headerlink" title="两类并行系统"></a>两类并行系统</h3><ol><li>Flynn 分类法<ol><li>根据指令流和数据流的概念对计算机的体系结构进行分类</li><li><img src="https://lh3.googleusercontent.com/-VqI58B-Y0RA/XDw8xAmCI7I/AAAAAAAANoQ/O9qHhKvwSMsZfN3gFs6CzSaKEyQmLNPfACHMYCw/s0/Acrobat_2019-01-14_15-39-48.png" alt=""></li></ol></li><li>SIMD 与MIMD 的最大区别<ol><li>SMID 使用一个控制器来控制多个处理器，而MIMD系统使用多个控制器异步地控制多个处理器</li><li>SIMD 中所有进程/线程执行完全相同的指令操作，而MIMD系统使用不同进程/线程执行不同的指令</li></ol></li><li>MIMD<ol><li>共享内存<ol><li>UMA结构（Uniform Memory Access）</li><li>NUMA结构（Non-Uniform Memory Access）</li><li>COMA结构（Cache-only Memory Access）</li></ol></li><li>分布内存</li></ol></li></ol><h3 id="互联网络"><a href="#互联网络" class="headerlink" title="互联网络"></a>互联网络</h3><ol><li>互联网络是：连接所有节点组成并行计算机的高速网络</li><li>两种互联网络：<ol><li>共享内存的互联网络（CPU通过互联网络与所需的Memory相连）<ol><li>总线（Buses）</li><li>交叉开关（Crossbars）</li></ol></li><li>分布内存的互联网络<ol><li>直接互联网络（两个节点直接相连）<ol><li>环</li><li>环绕网络</li><li>超立方</li></ol></li><li>间接互联网络（由开关网络负责处理节点之间的相连）<ol><li>交叉开关（Crossbars）</li><li>$\Omega$ 网络</li></ol></li></ol></li></ol></li><li>参数：<ol><li>延迟：是消息源开始収送消息到消息目的地接收到第一个字节的时间段。</li><li>带宽：是消息目的地接收第一个字节开始到完成数据接收，接收数据的速率。</li><li>理解：使用水龙头出水的时间来理解<ol><li>延迟：打开水龙头到出水的时延</li><li>带宽：水龙头口的大小</li></ol></li></ol></li></ol><h3 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h3><ol><li>概念<ol><li>指在含有多个Cache的并行系统中，数据的多个副本（因为没有同步更新）而造成的丌一致问题。</li></ol></li><li>更新缓存所需协议（二选一）<ol><li>写无效策略</li><li>写更新策略</li></ol></li><li>缓存一致性协议（二选一）<ol><li>监听总线协议</li><li>基于目录的协议</li></ol></li><li>伪共享<ol><li>现象：两个处理器上的线程，分别读取的两个不同的变量在同一个cache line里。</li><li><img src="https://lh3.googleusercontent.com/-KiiHq4K34Go/XDxANFBQAyI/AAAAAAAANoc/coRh3XIr92o-DRICB9c8857nFRItevs6gCHMYCw/s0/Acrobat_2019-01-14_15-54-29.png" alt=""></li></ol></li></ol><h2 id="超算软件"><a href="#超算软件" class="headerlink" title="超算软件"></a>超算软件</h2><p>主要有以下内容：</p><ol><li>共享内存如何协调？</li><li>分布内存如何协调？</li><li>混合编程</li></ol><h3 id="协调共享内存"><a href="#协调共享内存" class="headerlink" title="协调共享内存"></a>协调共享内存</h3><ol><li>动态线程，静态线程</li><li>不确定性</li><li>需要使用一些方法解决不确定性<ol><li>互斥锁</li><li>忙碌等待</li><li>信号量</li><li>等等</li></ol></li><li>线程安全<ol><li>代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。</li><li>例子：<code>strtok</code>函数</li></ol></li></ol><h3 id="协调分布内存"><a href="#协调分布内存" class="headerlink" title="协调分布内存"></a>协调分布内存</h3><ol><li>消息传递<ol><li><img src="https://lh3.googleusercontent.com/-qwebtN8Zvrg/XDxCVtr7avI/AAAAAAAANoo/8SFS1EwYUroyKg8HGGtJxk1UOGVZizb7wCHMYCw/s0/Acrobat_2019-01-14_16-03-35.png" alt=""></li></ol></li><li>单向通信（或称 远程内存访问）<ol><li>消息传递中，一个进程必须调用一个发送函数，并且必须与另一个进程调用的接受函数相匹配</li><li>问题：任何通信都需要两个进程的显式参与</li><li>解决：单向通信中，单个进程调用一个函数，可从其他进程中得到对应的值来更新局部内存，或者使用自己的值更新远端内存。这种通信，只需要一个进程的参与计科。</li><li><img src="https://lh3.googleusercontent.com/-jN2bOlWAYro/XDxC7MdwgiI/AAAAAAAANow/V0-wOHcfsGskmxmjA9dhv1j-Ve7vz3_TwCHMYCw/s0/Acrobat_2019-01-14_16-06-06.png" alt=""></li></ol></li><li>分区的全局地址空间<ol><li><img src="https://lh3.googleusercontent.com/-bSmqC6yOmMo/XDxDOvWazbI/AAAAAAAANo4/kBCUSEZV8PI70uGj9ZB3h7b-5I8gF62-gCHMYCw/s0/Acrobat_2019-01-14_16-07-23.png" alt=""></li></ol></li></ol><h2 id="编写并行程序"><a href="#编写并行程序" class="headerlink" title="编写并行程序"></a>编写并行程序</h2><p>主要有以下内容：</p><ol><li>一般步骤<ol><li>划分：大任务划分为小任务，使小任务可以并行执行。</li><li>通信：确定划分得到的小任务需要的通信。</li><li>集聚：如果小任务间有依赖关系，就把它们合并为一个任务。</li><li>映射：把小任务映射到丌同的迚程中，使得进程通信量最小且负载均衡。</li></ol></li><li>并行程序设计</li><li>编辑运行</li><li>输入输出</li><li>性能</li></ol><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><ol><li>计时<ol><li>需要关注CPU时间与真实运行时间的差异。</li></ol></li><li>加速比，效率<ol><li>加速比：串行计算时间与并行计算时间的比值<ol><li>$ \frac{T<em>{serial}}{T</em>{parallel}} ​$</li><li>线性加速比：计算速度随进程线程数的增加呈线性增长</li></ol></li><li>效率：加速比与进程数的比值<ol><li>$ E = \frac{S}{P} = \frac{T<em>{serial}}{p * T</em>{parallel}} $</li></ol></li><li>需要了解到，并行是有额外开销的</li></ol></li><li>阿姆达尔定律<ol><li>加速比是有上限的，无论如何增大处理器数目，加速比也无法高于某个数</li><li>$T_{serial} = W_s + W_p$</li><li>$T_{parallel} = W_s + W_p/p$</li><li>$S = \frac{W_s + W_p}{W_s + W_p/p}$<ol><li>当$p \rightarrow $正无穷的时候，上式具有极限</li></ol></li></ol></li><li>可扩展性<ol><li>同时增加问题规模和进程/线程数，并行程序的效率能基本保持丌变，就说这个程序是可扩展的。</li><li><strong>强可扩展</strong>：增加进程/线程数，为了维持效率而增加的问题规模不大</li><li><strong>弱可扩展</strong>：问题规模的增加的比率与进程/线程数增加比率一致（为了维持效率不变）</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HPC复习1-并行硬件与并行软件&quot;&gt;&lt;a href=&quot;#HPC复习1-并行硬件与并行软件&quot; class=&quot;headerlink&quot; title=&quot;HPC复习1-并行硬件与并行软件&quot;&gt;&lt;/a&gt;HPC复习1-并行硬件与并行软件&lt;/h1&gt;&lt;p&gt;主要分为四部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;背景介绍&lt;/li&gt;
&lt;li&gt;超算硬件&lt;/li&gt;
&lt;li&gt;超算软件&lt;/li&gt;
&lt;li&gt;编写并行程序&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/categories/High-Performance-Computing/"/>
    
    
      <category term="High Performance Computing" scheme="https://wwyf.github.io/tags/High-Performance-Computing/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统中的无层次命名</title>
    <link href="https://wwyf.github.io/2019/01/12/2019-01-2019-01-12-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%A0%E5%B1%82%E6%AC%A1%E5%91%BD%E5%90%8D/"/>
    <id>https://wwyf.github.io/2019/01/12/2019-01-2019-01-12-分布式系统中的无层次命名/</id>
    <published>2019-01-12T09:49:37.000Z</published>
    <updated>2019-01-12T09:57:41.499Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="分布式系统中的无层次命名"><a href="#分布式系统中的无层次命名" class="headerlink" title="分布式系统中的无层次命名"></a>分布式系统中的无层次命名</h1><p>今天整理了一下笔记，打算将无层次命名这一部分的笔记重新编辑一下，放到这里分享给大家。</p><h2 id="命名系统-基本概念"><a href="#命名系统-基本概念" class="headerlink" title="命名系统-基本概念"></a>命名系统-基本概念</h2><p>在分布式系统中，命名系统起着名称解析的作用，即，允许进程访问对应命名的实体。关于命名系统中的一些概念，这里需要弄清楚：</p><ol><li>实体与访问点的关系<ol><li>访问点是一个特殊的实体</li><li>通过访问点访问实体</li><li>实体可能具有多个访问点</li><li>实体的访问点可能会改变</li></ol></li><li><strong>名称</strong>：用于指向一个实体<ol><li><strong>位置无关</strong>的名称：独立于实体地址</li></ol></li><li><strong>地址</strong>：实体对应的某个<strong>访问点</strong>的名称</li><li><strong>标识符</strong>：用于唯一标识实体，实体与标识符是一对一的关系，且不可重用</li><li><strong>用户友好的名称</strong>：一般是字符串</li></ol><p>为了解决：把名称和标识符解析成地址 的问题，需要：</p><p><strong>名称到地址的绑定</strong></p><h2 id="无层次命名"><a href="#无层次命名" class="headerlink" title="无层次命名"></a>无层次命名</h2><blockquote><p>无层次命名是一种与实体空间位置无关的，扁平化的一种命名方法，一般用做实体的<strong>标识符</strong>。</p><p>名称解析：只给定实体的标识符（常用标识符做非结构化或无层次的名称），定位该实体。</p></blockquote><p>大体有四种方法：</p><ol><li><p>简单方法</p><ol><li>广播和多播<ol><li>包含该实体所用标识符的消息会通过广播发送到所有机器上，请求每一套机器检查它是否拥有该实体（实例：ARP 地址解析协议）</li></ol></li><li>转发指针（<strong>移动实体定位</strong>）<ol><li>每个转发指针都已（客户端存根，服务器存根）对的形式实现，当对象从地址空间A移动到地址空间B时，它会将一个客户存根留在A中，并且在B安装一个应用它的服务器存根。移动的细节对客户是透明的，客户可以顺着转发指针形成的链来查找实体对应的当前地址。</li><li>两种策略：<ol><li>直接向起始客户存根发送相应</li><li>按照转发指针的相反方向发送响应</li></ol></li></ol></li></ol></li><li><p>基于宿主位置</p><ol><li>所有与某主机地址的通信一开始都被转发到移动主机的宿主代理中。对移动主机来说，如果要转移到另一个网络，会获得一个新的地址，该<strong>转交地址</strong>要在宿主代理中注册<ol><li><img src="1547286768306.png" alt="1547286768306"></li></ol></li></ol></li><li><p>分布式散列表</p><ol><li><p>主要解决问题：m位的标识符$k$，解析为$succ(k)$的地址</p></li><li><p>如何高效解决：在每一个节点上维护一个指状表</p><ol><li><p><img src="1546940003091.png" alt="1546940003091"></p></li><li><p><img src="1546940017315.png" alt="1546940017315"></p></li></ol></li><li><p>加入与退出：</p><ol><li>节点p加入很简单，只需要请求$succ(p+1)$即可</li><li>更新指状表会比较复杂</li></ol></li></ol></li><li><p>分层方法</p><ol><li>目录节点：维护目录下的域所有实体的位置记录</li><li>低级的只有低级域的位置记录，高级域有多个低级域的位置记录</li><li>查询实体：自底向上，如果目录节点没有某记录，那转发到父节点继续查询</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;分布式系统中的无层次命名&quot;&gt;&lt;a href=&quot;#分布式系统中的无层
      
    
    </summary>
    
      <category term="Distributed System" scheme="https://wwyf.github.io/categories/Distributed-System/"/>
    
    
      <category term="Distributed System" scheme="https://wwyf.github.io/tags/Distributed-System/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://wwyf.github.io/2019/01/12/test-2018-hello-world/"/>
    <id>https://wwyf.github.io/2019/01/12/test-2018-hello-world/</id>
    <published>2019-01-12T07:36:03.302Z</published>
    <updated>2019-01-12T07:36:03.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="test"><a href="#test" class="headerlink" title="test"></a>test</h1><h2 id="test-imag"><a href="#test-imag" class="headerlink" title="test imag"></a>test imag</h2><p><img src="https://i.loli.net/2018/01/24/5a6875a4acc53.png" alt="这是使用公共图床上传的图片"></p><!-- ![](figure/2018-04-21-16-26-20.png) --><img src="/2019/01/12/test-2018-hello-world/2018-04-21-16-26-20.png" title="test"><h2 id="test-math"><a href="#test-math" class="headerlink" title="test math"></a>test math</h2><p>$$ a^2 = b $$</p><h2 id="test-chinese"><a href="#test-chinese" class="headerlink" title="test chinese"></a>test chinese</h2><p>这是中文。</p><h2 id="换了个头像"><a href="#换了个头像" class="headerlink" title="换了个头像"></a>换了个头像</h2><p><img src="http://blog.wwyf.top/logo.jpg" alt=""></p><h2 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h2><p>可以在<code>_posts</code>文件夹下创建自己的文件夹，然后hexo是不会管你的文件夹的！<br>继续测试。</p><p><img src="2018-05-05-11-31-29.png" alt="测试文件夹下放图片"></p><p>使用 typora的话，设置图片根目录后可以很方便的复制粘贴图片。</p><p><img src="1525494633350.png" alt="1525494633350"></p><h2 id="写好了一个脚本"><a href="#写好了一个脚本" class="headerlink" title="写好了一个脚本"></a>写好了一个脚本</h2><p>这个脚本用来自动创建一个新页面，并且填写yml模板信息</p><h2 id="测试脚注"><a href="#测试脚注" class="headerlink" title="测试脚注"></a>测试脚注</h2><p>脚注是<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="用来测试的脚注">[1]</span></a></sup></p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">用来测试的脚注<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;test&quot;&gt;&lt;a href=&quot;#test&quot; class=&quot;head
      
    
    </summary>
    
      <category term="test" scheme="https://wwyf.github.io/categories/test/"/>
    
    
      <category term="test" scheme="https://wwyf.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>配置博客</title>
    <link href="https://wwyf.github.io/2019/01/12/2019-01-2019-01-12-%E9%85%8D%E7%BD%AE%E5%8D%9A%E5%AE%A2/"/>
    <id>https://wwyf.github.io/2019/01/12/2019-01-2019-01-12-配置博客/</id>
    <published>2019-01-12T04:00:01.000Z</published>
    <updated>2019-01-15T16:42:10.387Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>今天配置了一下博客。</p><ol><li>本地仅维护markdown文件</li><li>通过git push，将markdown文件push到腾讯云服务器</li><li>云服务器中的远程git仓库触发hooks，cd到服务器的博客文件中，拉取最新博客文件，并执行hexo g -d 生成博客文件并发布</li></ol><p>最重要的一个改变在于：本地不需要存储博客的配置文件，仅需维护内容即可，一切配置文件都存放在了云服务器上，而且网页静态文件的生成也放在了云服务器上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;今天配置了一下博客。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;本地仅维护markdown文
      
    
    </summary>
    
      <category term="test" scheme="https://wwyf.github.io/categories/test/"/>
    
    
      <category term="test" scheme="https://wwyf.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>hexo配置评论系统</title>
    <link href="https://wwyf.github.io/2019/01/12/2018-05-2018-05-06-hexo%E9%85%8D%E7%BD%AE%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/"/>
    <id>https://wwyf.github.io/2019/01/12/2018-05-2018-05-06-hexo配置评论系统/</id>
    <published>2019-01-12T03:33:37.248Z</published>
    <updated>2019-01-12T03:33:37.248Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="hexo-评论系统"><a href="#hexo-评论系统" class="headerlink" title="hexo 评论系统"></a>hexo 评论系统</h1><p>有这样的想法，为自己的博客弄一个评论系统。</p><p>不过由于时间精力的缘故，还没有去弄。</p><p>先放一下要弄评论系统可能需要的一些资料。</p><p>第三方的评论系统似乎都不太好使，打算自建</p><p><a href="http://www.candura.us/posts/post-348/" target="_blank" rel="noopener">http://www.candura.us/posts/post-348/</a></p><p><a href="https://wzfou.com/hashover/" target="_blank" rel="noopener">https://wzfou.com/hashover/</a></p><p><a href="https://zhuanlan.zhihu.com/p/26955370" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26955370</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;hexo-评论系统&quot;&gt;&lt;a href=&quot;#hexo-评论系统&quot; c
      
    
    </summary>
    
      <category term="test" scheme="https://wwyf.github.io/categories/test/"/>
    
    
      <category term="test" scheme="https://wwyf.github.io/tags/test/"/>
    
      <category term="hexo" scheme="https://wwyf.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>hexo中的评论系统</title>
    <link href="https://wwyf.github.io/2019/01/11/2019-01-2019-01-11-hexo%E4%B8%AD%E7%9A%84%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/"/>
    <id>https://wwyf.github.io/2019/01/11/2019-01-2019-01-11-hexo中的评论系统/</id>
    <published>2019-01-11T13:55:16.000Z</published>
    <updated>2019-01-12T03:33:37.284Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="关于hexo的评论系统"><a href="#关于hexo的评论系统" class="headerlink" title="关于hexo的评论系统"></a>关于hexo的评论系统</h1><p>这里要推荐一个极简无后端的评论系统！！！</p><blockquote><p>Valine 诞生于2017年8月7日，是一款基于Leancloud的快速、简洁且高效的无后端评论系统。<br>理论上支持但不限于静态博客，目前已有Hexo、Jekyll、Typecho、Hugo 等博客程序在使用Valine。</p></blockquote><p>官网是这一个：<a href="https://valine.js.org/" target="_blank" rel="noopener">https://valine.js.org/</a></p><h2 id="使用感受"><a href="#使用感受" class="headerlink" title="使用感受"></a>使用感受</h2><p>到leancloud上创建一个应用，然后找到把appid和appkey填到hexo的config里就好了！别的什么都不用怎么配置，哇比其他的方便多了，特别是之前那一个已经没有人维护的gitment。</p><p>引用几个博客的链接：</p><p><a href="https://blog.csdn.net/esa_dsq/article/details/78626509" target="_blank" rel="noopener">https://blog.csdn.net/esa_dsq/article/details/78626509</a><br><a href="https://xiaotiandi.github.io/publicBlog/2018-09-19-d196c9ad.html" target="_blank" rel="noopener">https://xiaotiandi.github.io/publicBlog/2018-09-19-d196c9ad.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;关于hexo的评论系统&quot;&gt;&lt;a href=&quot;#关于hexo的评论系
      
    
    </summary>
    
      <category term="test" scheme="https://wwyf.github.io/categories/test/"/>
    
    
      <category term="test" scheme="https://wwyf.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>立一个flag</title>
    <link href="https://wwyf.github.io/2019/01/11/2019-01-2019-01-11-%E7%AB%8B%E4%B8%80%E4%B8%AAflag/"/>
    <id>https://wwyf.github.io/2019/01/11/2019-01-2019-01-11-立一个flag/</id>
    <published>2019-01-11T13:10:01.000Z</published>
    <updated>2019-01-15T16:45:37.991Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>test 目录就是让我用来随便测试的吧。</p><p>吐槽一下，现在考试进度 6/8，加油吧~</p><p>我想测试一个图片</p><p><img src="1547212329062.png" alt="1547212329062"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;test 目录就是让我用来随便测试的吧。&lt;/p&gt;
&lt;p&gt;吐槽一下，现在考试进
      
    
    </summary>
    
      <category term="test" scheme="https://wwyf.github.io/categories/test/"/>
    
    
      <category term="test" scheme="https://wwyf.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>BCNF与4NF</title>
    <link href="https://wwyf.github.io/2018/11/01/2018-11-2018-11-01-BCNF%E4%B8%8E4NF/"/>
    <id>https://wwyf.github.io/2018/11/01/2018-11-2018-11-01-BCNF与4NF/</id>
    <published>2018-11-01T02:18:23.000Z</published>
    <updated>2019-01-12T09:07:41.134Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="数据库关系模式的BCNF分解与4NF分解"><a href="#数据库关系模式的BCNF分解与4NF分解" class="headerlink" title="数据库关系模式的BCNF分解与4NF分解"></a>数据库关系模式的BCNF分解与4NF分解</h1><p>这两种分解看得我云里雾里，今早好不容易觉得看懂了，觉得要写成一篇blog记下来，不然以后回来再看的时候可能又要看半天才能看懂了o(╥﹏╥)o。</p><p>书本上说的会比较抽象，虽然这保证了定义和方法的准确性，但是要去理解实在是有点困难，我想，将我解答题目的过程放上来能帮助这几种分解的理解。</p><h2 id="BCNF分解实例"><a href="#BCNF分解实例" class="headerlink" title="BCNF分解实例"></a>BCNF分解实例</h2><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>有这样的关系模式$r(A,B,C,D,E,F)$，其中该关系模式需要满足以下函数依赖：</p><p>$ A \rightarrow BCD $</p><p>$ BC \rightarrow DE $</p><p>$ B \rightarrow D $</p><p>$ D \rightarrow A $</p><h3 id="分析原理"><a href="#分析原理" class="headerlink" title="分析原理"></a>分析原理</h3><ol><li>如何判断一个关系模式是否满足BCNF模式<ol><li>书本P196最上面：一个最简单的判定方法，但不可用于分解后的关系模式的判定</li><li>书本P196中间：可用于分解后的关系模式的判定</li></ol></li></ol><p>这两种方法，我觉得后者会相对比较难理解，但是在实际题目中还是的确能够使用上的。</p><p>具体的定义书本上就有，这里也照抄一份：</p><p>第一种方法：</p><p><img src="http://bit.ly/2Oka04m" alt=""></p><p>第二种方法：</p><p><img src="http://bit.ly/2Og6kkd" alt=""></p><h3 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h3><p>就，按顺序一个函数依赖一个函数依赖的看就好了。</p><h4 id="第一步分解"><a href="#第一步分解" class="headerlink" title="第一步分解"></a>第一步分解</h4><p>目前的关系模式$r$还没有分解，就使用第一种方法来进行分析：</p><p>对于函数依赖$A \rightarrow BCD$来说，根据第一种方法，我先计算$A^{+} = ABCDE$，发现$A^{+}$并没有包含关系$r$中的所有属性（$ABCDEF$，少了个$F$），因此$A$不是关系模式$r$的超码。</p><p>这样子，我就根据函数依赖$A \rightarrow BCD$说明了关系模式$r$不属于BCNF，因此将原有的关系模式$r$分解为$ (r-BCD) \cup (A,BCD) $</p><p>因此此步分解得到以下关系模式：</p><blockquote><p>$ r_1(A,B,C,D) $<br>$ r_2(A,E,F) $</p></blockquote><h4 id="检验第一步分解结果"><a href="#检验第一步分解结果" class="headerlink" title="检验第一步分解结果"></a>检验第一步分解结果</h4><p>在第二步分解结果前，需要检验第一步的分解结果是否满足BCNF条件，注意到这两个关系模式是分解后产生的，原先的第一种用来判断关系模式是否属于BCNF的方法不能够再使用，后面均使用第二种方法。</p><p>先看关系模式$r_1(A,B,C,D)$。</p><p>第二种方法要求$r_1$中属性的每一个子集$\alpha$，确保$\alpha^{+}$（F下$\alpha$的属性闭包）要么不包含$r-\alpha$的任何属性，要么包含$r_1$的所有属性。</p><blockquote><p>这里为遍历$r_1$中属性子集的过程<br>对于属性$A$，$A^{+} = ABCDE$，包含了$r_1$的所有属性<br>对于属性$B$，$B^{+} = ABCDE$，包含了$r_1$的所有属性<br>对于属性$C$，$C^{+} = C$，不包含$r_1 - C$的任何属性<br>对于属性$D$，$D^{+} = ABCDE$,包含了$r_1$的所有属性<br>对于属性$E$，$E^{+} = E$，不包含$r_1 - E$的任何属性<br>对于属性子集$AB，AC，AD，AE，BC，BD，BE，CD，DE$，属性闭包均为$ABCDE$，包含了$r_1$的所有属性<br>对于属性子集$CD$，$CD^{+} = CD$，不包含$r_1 - C$的任何属性<br>对于三个属性以上的属性子集，属性闭包均为$ABCDE$，包含了$r_1$的所有属性</p></blockquote><p>综上，$r_1$满足BCNF条件。</p><p>再看关系模式$r_2(A,E,F)$:</p><blockquote><p>这里为遍历$r_2$属性子集的过程<br>对于属性$A$，$A^{+} = ABCDE$，而$r_2 - A = EF$，会发现$A^{+}$包含了$r_2 - A$中的属性$E$，且没有包含$r_2$的所有属性（如属性$F$）</p></blockquote><p>找到了一个属性违反该条件，那么就可以证明有这样的一个函数依赖出现在$F^{+}$中：</p><p>$$A \rightarrow (A^{+} - A) \cap r_2$$</p><p>算一算，$A^{+} - A \cap r_2 = BCDE \cap AEF = E$</p><p>因此便找到了这样的一个函数依赖$A \rightarrow E$，让$r_2$不满足BCNF</p><h4 id="第二步分解"><a href="#第二步分解" class="headerlink" title="第二步分解"></a>第二步分解</h4><p>上面找到了这样的一个函数依赖$A \rightarrow E$，让$r_2(A,E,F)$不满足BCNF，<br>因此$r_2$可以这样子分解：</p><p>$r_3(A, E)$</p><p>$r_4(A, F)$</p><p>此步骤得到的分解结果为</p><blockquote><p>$r_1(A,B,C,D)$<br>$r_3(A, E)$<br>$r_4(A, F)$</p></blockquote><h4 id="检验第二步结果"><a href="#检验第二步结果" class="headerlink" title="检验第二步结果"></a>检验第二步结果</h4><p>$r_1$上面已经检验过了，这里只需要检验$r_3$和$r_4$即可。</p><p>对于关系模式$r_3$：</p><blockquote><p>对于属性$A$， $A^{+} = ABCDE$，包含了$r_3$的所有属性<br>对于属性$E$，$E^{+} = E$，不包含$r_3 - E$的任何属性</p></blockquote><p>对于关系模式$r_4$：</p><blockquote><p>对于属性$A$，$A^{+} = ABCDE$，而$r_4 - A = F$，$A^{+}$不包含$r_4 - A$的所有属性<br>对于属性$F$，$F^{+} = F$，不包含$r_3 - F$的任何属性</p></blockquote><p>综上，关系模式$r_3$,$r_4$，都已经满足BCNF条件</p><h4 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h4><p>已经检验过了，每一个关系模式都满足BCNF条件，因此，最终结果便是：</p><blockquote><p>$r_1(A,B,C,D)$<br>$r_3(A, E)$<br>$r_4(A, F)$</p></blockquote><h2 id="3NF分解实例"><a href="#3NF分解实例" class="headerlink" title="3NF分解实例"></a>3NF分解实例</h2><p>我觉得3NF比BCNF简单很多，不想写了hhh</p><h2 id="4NF分解实例"><a href="#4NF分解实例" class="headerlink" title="4NF分解实例"></a>4NF分解实例</h2><h3 id="分解原理"><a href="#分解原理" class="headerlink" title="分解原理"></a>分解原理</h3><p>关于4NF，其实课本P201上已经说明了检验模式是否满足4NF的方法。</p><p>一种方法便是4NF的定义，定义可以好好看看书本，和BCNF的定义类似，但问题在于这一个定义无法用于分解后的关系模式中。</p><p><img src="http://bit.ly/2OcpJ5v" alt=""></p><p>第二种方法可以用在分解后的关系模式，其实就是找到在分解后的关系模式上的限定$D_i$，然后对于该限定$D_i$里面的每一个依赖，都使用4NF的定义去检查是否满足条件。</p><p><img src="http://bit.ly/2Ohgf95" alt=""></p><h3 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h3><p>$R = (A,B,C,G,H,I)$<br>$F = {$<br>$A \rightarrow\rightarrow B$,<br>$B \rightarrow\rightarrow HI$,<br>$CG \rightarrow\rightarrow H$<br>$}$</p><h3 id="分解流程"><a href="#分解流程" class="headerlink" title="分解流程"></a>分解流程</h3><h4 id="第一步分解-1"><a href="#第一步分解-1" class="headerlink" title="第一步分解"></a>第一步分解</h4><p>使用第一种方法来判断，注意到多值依赖是一种比函数依赖更弱的依赖，因此这里我觉得较难判断关系模式$R$的超码，就暂且认为该关系模式中的每一个属性都不是超码。</p><p>因为$A \rightarrow\rightarrow B$满足，且$A$不是关系模式$R$的超码，因此分解$R$得到</p><blockquote><p>$R_1(A,B)$<br>$R_2(A,C,G,H,I)$</p></blockquote><h4 id="判断第一步分解结果"><a href="#判断第一步分解结果" class="headerlink" title="判断第一步分解结果"></a>判断第一步分解结果</h4><p>判断分解后的关系模式是否满足4NF条件，需要使用第二种方法，这里需要计算函数依赖和多值依赖的集合$D$在$R_1$和$R_2$上的限定。</p><h5 id="判断1"><a href="#判断1" class="headerlink" title="判断1"></a>判断1</h5><p>先判断关系模式$R_1(A,B)$，我通过如下方式寻找函数依赖和多值依赖的集合$D$在$R_1$上的限定$D_1$:</p><blockquote><ol><li>$D^{+}$中所有只含有$R_1$中属性的函数依赖：无</li><li>所有形如$\alpha \rightarrow\rightarrow \beta \cap R_1$的多值依赖，其中$\alpha \in R_1$，且$\alpha \rightarrow\rightarrow \beta$ 属于$D^{+}$:<blockquote><p>当$\alpha = A$，能够找到$A\rightarrow\rightarrow B$，还有$A\rightarrow\rightarrow HI$，使用$\beta \cap R_1$处理后得到仅有$A\rightarrow\rightarrow B$在限定$D_1$中<br>当$\alpha = B$，能够找到$B\rightarrow\rightarrow HI$，使用$\beta \cap R_1$处理后发现没有多值依赖在限定$D_1$中</p></blockquote></li></ol></blockquote><p>因此函数依赖和多值依赖的集合$D$在$R_1$上的限定$D_1$为${ (A\rightarrow\rightarrow B) }$。</p><p>接下来判断限定里面的依赖是否能够让关系模式$R_1$满足4NF条件。因为在关系模式$R_1$中仅有两个属性$A,B$，因此多值依赖$A\rightarrow\rightarrow B$是一个平凡的多值依赖。因此满足4NF条件。</p><h5 id="判断2"><a href="#判断2" class="headerlink" title="判断2"></a>判断2</h5><p>判断关系模式$R_2(A,C,G,H,I)$是否满足4NF条件。</p><p>同样是两个步骤，显示找到函数依赖和多值依赖的集合$D$在$R_2$上的限定$D_2$，然后在限定$D_2$中，遍历每一个依赖关系，寻找是否有使得$R_2$不满足条件的依赖关系，若有则不满足4NF条件，若无则该关系模式满足4NF条件。</p><ol><li>在限定$D_2$中，我找到了这样的依赖关系$CG \rightarrow\rightarrow H$</li><li>该依赖关系并不平凡，并且$CG$也不是$R_2$的一个超码</li></ol><p>因此关系模式$R_2$不满足4NF条件，需要分解</p><h4 id="第二步分解-1"><a href="#第二步分解-1" class="headerlink" title="第二步分解"></a>第二步分解</h4><p>前面说到，根据依赖关系$CG \rightarrow\rightarrow H$，$R_2(A,C,G,H,I)$并不满足4NF条件。</p><p>因此进行分解得到以下关系模式</p><p>$R_3(C,G,H)$<br>$R_4(A,C,G,I)$</p><h4 id="判断第二步分解结果是否满足4NF条件"><a href="#判断第二步分解结果是否满足4NF条件" class="headerlink" title="判断第二步分解结果是否满足4NF条件"></a>判断第二步分解结果是否满足4NF条件</h4><p>然后又需要判断$R_3$,$R_4$是否满足4NF条件。</p><h5 id="判断-R-3"><a href="#判断-R-3" class="headerlink" title="判断$R_3$"></a>判断$R_3$</h5><p>判断仍然是两步走，先寻找函数依赖和多值依赖的集合$D$在$R_3$上的限定$D_3$，然后遍历该限定$D_3$中的每一个依赖关系。</p><ol><li>$D_3 = { (CG \rightarrow\rightarrow H) }$</li><li>$D_3$中仅有一个依赖关系，且$CG \rightarrow\rightarrow H$是一个平凡的多值依赖（因为$CG \cap H = R_3$），满足4NF条件</li></ol><p>因此$R_3$满足4NF条件</p><h5 id="判断-R-4"><a href="#判断-R-4" class="headerlink" title="判断$R_4$"></a>判断$R_4$</h5><p>对$R_4(A,C,G,I)$一样的方法，寻找$D_4$,然后遍历$D_4$。这里我会更详细地说明限定$D_4$的计算方法</p><blockquote><ol><li>$D^{+}$中所有只含有$R_4(A,C,H,I)$中属性的函数依赖：无</li><li>所有形如$\alpha \rightarrow\rightarrow \beta \cap R_4$的多值依赖，其中$\alpha \in R_4$，且$\alpha \rightarrow\rightarrow \beta$ 属于$D^{+}$:<blockquote><p>当$\alpha = A$，能够找到$A\rightarrow\rightarrow B$，还有$A\rightarrow\rightarrow HI$，使用$\beta \cap R_1$处理后得到仅有$A\rightarrow\rightarrow I$在限定$D_4$中<br>当$\alpha = C$，或$\alpha = H$ 或$\alpha = I$，均找不到符合条件的多值依赖<br>当$\alpha = AC$,$\alpha = AH$,$\alpha = AI$,等等等，所有子集都找不到符合条件的多值依赖</p></blockquote></li></ol></blockquote><p>因此$D_4 = { (A \rightarrow\rightarrow I) }$</p><p>对着一个函数依赖我会发现，$A$并不是关系模式$R_4$的主键，因此该关系模式不符合4NF条件。</p><h4 id="第三步分解"><a href="#第三步分解" class="headerlink" title="第三步分解"></a>第三步分解</h4><p>上面找到了$A \rightarrow\rightarrow I$让关系模式$R_4$不满足4NF条件，因此分解成以下两个关系模式</p><p>$R_5(A,I)$</p><p>$R_6(A,C,G)$</p><h4 id="判断第三步结果是否符合4NF条件"><a href="#判断第三步结果是否符合4NF条件" class="headerlink" title="判断第三步结果是否符合4NF条件"></a>判断第三步结果是否符合4NF条件</h4><p>判断的过程依然是两步走</p><h5 id="判断-R-5"><a href="#判断-R-5" class="headerlink" title="判断$R_5$"></a>判断$R_5$</h5><p>计算得到$D_5 = { (A\rightarrow\rightarrow I) }$</p><p>该依赖关系在$R_5$中是平凡的，因为$A \cap I = R_5$</p><p>因此$R_5$满足4NF条件</p><h5 id="判断-R-6"><a href="#判断-R-6" class="headerlink" title="判断$R_6$"></a>判断$R_6$</h5><p>计算$D_6$:</p><blockquote><ol><li>$D^{+}$中所有只含有$R_6(A,C,G)$中属性的函数依赖：无</li><li>所有形如$\alpha \rightarrow\rightarrow \beta \cap R_6$的多值依赖，其中$\alpha \in R_6$，且$\alpha \rightarrow\rightarrow \beta$ 属于$D^{+}$:<blockquote><p>当$\alpha = A$，能够找到$A\rightarrow\rightarrow B$，还有$A\rightarrow\rightarrow HI$，使用$\beta \cap R_1$处理后得找不到符合条件的多值依赖<br>当$\alpha = C$，或$\alpha = G$均找不到符合条件的多值依赖<br>当$\alpha = AC$,$\alpha = AG$,$\alpha = CG$,等等等，能够找到$CG \rightarrow\rightarrow H$，但是在使用$\beta \cap R_6$后，依然是空集</p></blockquote></li></ol></blockquote><p>因此，$D_6 = \emptyset$。</p><p>这个时候也可以说，对$D_6$中所有依赖，均满足4NF条件。</p><p>因此$R_6$满足4NF条件</p><h4 id="最终分解的结果"><a href="#最终分解的结果" class="headerlink" title="最终分解的结果"></a>最终分解的结果</h4><p>最终分解得到</p><blockquote><p>$R_1(A,B)$<br>$R_3(C,G,H)$<br>$R_5(A,I)$<br>$R_6(A,C,G)$</p></blockquote><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>总之就是严格按照课本定义，一点一点地推导和证明。</p><p>课本上的定义和方法实在是太过抽象了，我希望自己可以通过对这一些实例的详细探讨，对这一些分解方法有一个比较清晰的认识就好，昨天做作业的时候还是感觉自己迷迷糊糊的，现在就感觉，注意到了一些昨天没有注意到的细节，然后对书本这些理论的自洽性也有了一些比较深的理解，本来一些觉得诶好像证明不了的，回去一看看定义，哦原来是我对定义本身就不清楚……诸如此类的问题还是蛮多的.啊希望以后期末的时候对这一块的内容可以通过这篇博客更好地复习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;数据库关系模式的BCNF分解与4NF分解&quot;&gt;&lt;a href=&quot;#数
      
    
    </summary>
    
      <category term="Datebase" scheme="https://wwyf.github.io/categories/Datebase/"/>
    
    
      <category term="Datebase" scheme="https://wwyf.github.io/tags/Datebase/"/>
    
  </entry>
  
  <entry>
    <title>关于word-embedding的理解</title>
    <link href="https://wwyf.github.io/2018/07/28/2018-07-2018-07-28-%E5%85%B3%E4%BA%8Eword-embedding%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://wwyf.github.io/2018/07/28/2018-07-2018-07-28-关于word-embedding的理解/</id>
    <published>2018-07-28T02:00:47.000Z</published>
    <updated>2019-01-12T03:33:37.284Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="关于word-embedding的理解与pytorch实现"><a href="#关于word-embedding的理解与pytorch实现" class="headerlink" title="关于word-embedding的理解与pytorch实现"></a>关于word-embedding的理解与pytorch实现</h1><p>在NLP中，计算机需要一种方法，表示一个单词。我们马上可以想到，可以直接使用ascii来保存呀，计算机也能够识别。</p><p>用ascii并不是不可以，但是，大家都知道，如果能够把单词表示成向量的形式，无论在怎样的数学处理中都会更加方便，即使是用ascii，最终也必须在计算机中有一种数学的方法表达，才能够完成后续的语义识别的工作。这种，为单词寻找到一种在计算机中表示的方法，可以称之为”Word embedding”(这里的定义是不完整的，后面继续补充。)</p><p>那么，什么办法可以表示一个单词呢？一种显而易见的办法是使用“one-hot encoding”,也就是说，每一个单词，在这样的一个向量中，都有一个独一不二的索引。对不同的词，便在不同的位置为1，其余的位置为0.</p><p><img src="http://bit.ly/2mNgfSX" alt=""></p><p>不妨将单词向量化后的那一个向量空间称之为单词空间？那么，对于单词空间中的每一个具有“one-hot encoding”性质的向量，我们都能够找到一个单词一一对应。</p><p>“one-hot encoding”已经解决了单词-单词空间向量的映射问题，但是，似乎不太好呢。大家都知道，每一个单词之间都有着或多或少的关联。<strong>one-hot encoding方法，将单词之间可能具有的关联信息全部都抛弃掉，只留下一一对应的性质</strong>，这样的单词嵌入方法，并不是特别的理想。</p><h2 id="真正的word-embedding的定义"><a href="#真正的word-embedding的定义" class="headerlink" title="真正的word-embedding的定义"></a>真正的word-embedding的定义</h2><p>摘自知乎</p><blockquote><p>Embedding在数学上表示一个maping, f: X -&gt; Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 &lt; X2,那么映射后在Y所属空间上同理 Y1 &lt; Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。</p></blockquote><p>个人觉得这位网友说的还是很不错的。word embedding的关键在于两点</p><ol><li>单射</li><li><strong>结构保存</strong>（原本关联度较大的两个词，在新的向量空间中的相似度也应该较大？）</li></ol><h2 id="word-embedding的输入输出"><a href="#word-embedding的输入输出" class="headerlink" title="word-embedding的输入输出"></a>word-embedding的输入输出</h2><p>输入：一个单词<br>输出：这一个单词对应的向量</p><p>其中为了达到<strong>结构保存</strong>的目的，前期还需要很多的语料库进行训练。</p><h2 id="pytorch中的word-embedding"><a href="#pytorch中的word-embedding" class="headerlink" title="pytorch中的word embedding"></a>pytorch中的word embedding</h2><p>pytorch中预先已经提供了很多可用了“神经元层”，其中有一个<code>nn.Embedding(a,b)</code>就是专门用于完成“word embedding”工作的神经元层。该层的功能是：将一个具有$a$个单词的字典中的所有单词，映射到一个$b$维的向量空间中。</p><p>简单的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">word_to_ix = &#123;<span class="string">"hello"</span>: <span class="number">0</span>, <span class="string">"world"</span>: <span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)  <span class="comment"># 2 words in vocab, 5 dimensional embeddings</span></span><br><span class="line">lookup_tensor = torch.tensor([word_to_ix[<span class="string">"hello"</span>]], dtype=torch.long)</span><br><span class="line"><span class="comment"># print(lookup_tensor)  # tensor([0])</span></span><br><span class="line">hello_embed = embeds(lookup_tensor) <span class="comment"># 输入单词索引即可</span></span><br><span class="line">print(hello_embed)</span><br></pre></td></tr></table></figure><h2 id="An-Example-N-Gram-Language-Modeling"><a href="#An-Example-N-Gram-Language-Modeling" class="headerlink" title="An Example: N-Gram Language Modeling"></a>An Example: N-Gram Language Modeling</h2><p>([ word_i-2, word_i-1 ], target word)</p><p>官网上提供了一个根据上文预测下一个单词的神经网络以供参考。代码我就不放上来了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>pytorch官方教程 <a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</a></li><li>一个知乎的回答：<a href="https://www.zhihu.com/question/32275069/answer/80188672" target="_blank" rel="noopener">https://www.zhihu.com/question/32275069/answer/80188672</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;关于word-embedding的理解与pytorch实现&quot;&gt;&lt;a
      
    
    </summary>
    
      <category term="Machine-Learning" scheme="https://wwyf.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine-Learning" scheme="https://wwyf.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
